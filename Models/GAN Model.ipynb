{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(\"Pokemon MIDIs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "            \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab, seq_length):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = seq_length\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # Normalize input between -1 and 1\n",
    "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = numpy.random.randint(0, len(network_input)-1)\n",
    "    \n",
    "    # Get pitch names and store in a dictionary\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern = numpy.append(pattern,index)\n",
    "        #pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for item in prediction_output:\n",
    "        pattern = item[0]\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='{}.mid'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, rows):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 1)\n",
    "        self.latent_dim = 1000\n",
    "        self.disc_loss = []\n",
    "        self.gen_loss =[]\n",
    "        \n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates note sequences\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated_seq = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated_seq)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
    "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        seq = Input(shape=self.seq_shape)\n",
    "        validity = model(seq)\n",
    "\n",
    "        return Model(seq, validity)\n",
    "      \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.seq_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load and convert the data\n",
    "        notes = get_notes()\n",
    "        n_vocab = len(set(notes))\n",
    "        X_train, y_train = prepare_sequences(notes, n_vocab, self.seq_length)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Select a random batch of note sequences\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_seqs = X_train[idx]\n",
    "\n",
    "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
    "            #noise = (noise-242)/242\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new note sequences\n",
    "            gen_seqs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            #  Training the Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, real)\n",
    "\n",
    "            # Print the progress and save into loss lists\n",
    "            if epoch % sample_interval == 0:\n",
    "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "              self.disc_loss.append(d_loss[0])\n",
    "              self.gen_loss.append(g_loss)\n",
    "        \n",
    "        self.generate(notes)\n",
    "        self.plot_loss()\n",
    "        \n",
    "    def generate(self, input_notes):\n",
    "        # Get pitch names and store in a dictionary\n",
    "        notes = input_notes\n",
    "        pitchnames = sorted(set(item for item in notes))\n",
    "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "        \n",
    "        # Use random noise to generate sequences\n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "        predictions = self.generator.predict(noise)\n",
    "        \n",
    "        pred_notes = [x*242+242 for x in predictions[0]]\n",
    "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
    "        \n",
    "        create_midi(pred_notes, 'gan_final')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.disc_loss, c='red')\n",
    "        plt.plot(self.gen_loss, c='blue')\n",
    "        plt.title(\"GAN Loss per Epoch\")\n",
    "        plt.legend(['Discriminator', 'Generator'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_7 (CuDNNLSTM)     (None, 50, 512)           1054720   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 1024)              4202496   \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,913,601\n",
      "Trainable params: 5,913,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 50)                51250     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 50, 1)             0         \n",
      "=================================================================\n",
      "Total params: 971,570\n",
      "Trainable params: 967,986\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Parsing Pokemon MIDIs\\Pokemon - Farewell, Pikachu!.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Lavender town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Littleroot Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Lugias Song.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Oracion.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Pallet Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Pkmn Elite 4.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - pokecentre theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Pokemon Center Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Pokemon Center.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - Pokemon Johto - Opening.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon - The Ghost at Maiden's Peak.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Black & White - Village Bridge.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Black 2White 2 - Cave of Being.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Black 2White 2 - Join Avenue  Greeting.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Black 2White 2 - Ns Room.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Black 2White 2 - Ns Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Black 2White 2 - THE END.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Accumula Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - An Unwavering Heart.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Battle CherenBianca.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Battle Elite Four.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Battle Legendary Pokemon.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Battle Team Plasma.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Battle Wild Pokemon.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Bicycle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Castelia City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Champion Alder.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Crisis in Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Dragonspiral Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Driftveil City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Ending Onward to Our Own Futures.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Icirrus City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Lacunosa Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Mistralton City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Musical Hall.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - N the Pokemon Child.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Nacrene City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Ns Castle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Ns Farewell.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Nuvema Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Opelucid City White.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Pokemon League.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Relic Song.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Route 10.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Route 3.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Route 4.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Route 6.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Skyarrow Bridge.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Striaton City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Surfing.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Undella Town FallWinterSpring.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Undella Town Summer.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - Village Bridge.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon BlackWhite - White Forest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Channel - Pokepad.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - Miror B Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - Outskirt Stand.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - Pyrite Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - Relic Forest in Danger.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - Relic Forest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - Semifinal Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Colosseum - The Under.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Approaching Champion Cynthia.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Azelf Mesprit and Uxie Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Battle Champion.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Battle Rival.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Battle Team Galactic Admin.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Battle Trainer.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Battle Wild Pokemon.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Canalave City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Cyrus  Team Galactic Boss Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - DialgaPalkia Appear.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Elite Four Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Eterna City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Floaroma Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Game Corner.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Hearthome City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Jubilife City Day.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Lake Caverns.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Mt Coronet.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Mystery Gift.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Old Chateau.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Pokemon League Day.mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Poketch Addition.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Research Lab.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Route 202.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Route 205 Night.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Route 205.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Route 209.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Route 225 Night.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Route 228 Night.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Sandgem Town Day.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Spear Pillar.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Stark Mountain.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Team Galactic Grunt Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Torn WorldDistortion World.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Twinleaf Town Day.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Veilstone City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - Villa  Twinleaf Music Box.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon DiamondPearlPlatinum - WiFi Lobby.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Gold, Silver, Crystal - Cinnabar Island (HGSS Version).mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Gold, Silver, Crystal - S.S. Aqua .mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Azalea TownBlackthorn City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Bicycle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Bug Catching Contest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Burned Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Champion Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Cherrygrove CityMahogany Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Dance Theatre.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Dark Cave.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Dragons Den.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Ecruteak CityCianwood City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Girl Trainer Confrontation.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Goldenrod City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Gym Leader Defeated.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Gym.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Indigo Plateau.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Johto Gym Leader Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Johto Wild Pokemon Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Kanto Gym Leader Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Lavender Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Menu.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - National Park.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - New Bark Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Olivine Lighthouse.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Pokemon Lullaby.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Pokemon March.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Professor Elms Lab.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Professor Oaks Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Rival Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Route 27.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Route 29.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Route 32.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Route 38.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Route 42.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Saffron City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Show Me Around.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Sprout Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Staff Credits.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Surf.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Team RocketRadio Tower Takeover.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Tin Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Title.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Union Cave.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Victory Road.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon GoldSilverCrystal - Violet CityOlivine City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Cerulean City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Cinnabar Island.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Game Corner.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Indigo Plateau.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Instructions.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Lyras Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - New Bark Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Pokemaniac Encounter.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Pokemon Center.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - PokeWalker Synchronisation.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Route 4748.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Safari Zone Gate.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Team Rocket Hideout.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Title.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon HeartGoldSoulSilver - Viridian Forest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon - Red Rescue Team - Pokemon Square.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Escape Through the Snow  Snow Refugees.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Friend Area  Swamp.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Great Canyon.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Monster House.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Parting.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Questionnaire.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Rescue Team Base.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Sinister Woods.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Sky Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Tiny Woods.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon BlueRed Rescue Team - Title Screen.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - A Sinister Smell Team Skulls Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - A Wish For Peace.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Aegis Cave.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Amp Plains.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Apple Woods.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - At the End of the Day.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Defend Globe.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Drenched Bluff.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Dusk Forest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Ending Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Epilogue Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Foggy Forest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Goodnight.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Guildmaster Wigglytuff.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Heartwarming.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Hidden Highland.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Hidden Land.mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - I Dont Want to Say Goodbye.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - In the Hands of Fate.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Lower Brine Cave.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Oh No.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Palkias Onslaught.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Pelippers Island.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Sealed Ruin Pit.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Sky Peak Final Pass.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Spacial Cliffs.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Spindas Cafe.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Steam Cave.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Team Charms Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Temporal Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - The Gatekeepers.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Time Gear Remix.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Explorers of TimeDarknessSky - Title Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Gates to Infinity - Desperation Light Arrangement.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Mystery Dungeon Gates to Infinity - File Select.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Omega RubyAlpha Sapphire - Leaders Theme Team MagmaTeam Aqua.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Omega RubyAlpha Sapphire - Strains of a New Beginning Theme of rs.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Omega RubyAlpha Sapphire - The Lament of Falling Stars.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Omega RubyAlpha Sapphire - Trainers Eyes Meet Poke Fan.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Pinball - Catch Em and Evolution Mode in Red Field.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Pinball - Field Select.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Pinball - Pokedex.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Pinball - Red Field Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Ranger - Ranger Base.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Celadon City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Cerulean City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Champion Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Cinnabar Island.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Cycling.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Elite Four.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Ending Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Evolution.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Game Corner Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Gym Leader Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Gym.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Hall of Fame.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Indigo Plateau.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Intro 1.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Intro 2.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Jigglypuffs Song.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Lavender Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Mt Moon.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Oaks Lab.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Opening Yellow.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Pallet Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Pokemon Center.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Pokemon Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Printer Error.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Route 1.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Route 12.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Route 24.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Route 3.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Show Me Around.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Silph Co.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - SS Anne.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Surf Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Team Rocket Hideout.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Title Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Trainer Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Vermillion City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Viridian City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Viridian Forest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RedBlueYellow - Wild Pokemon Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Abandoned Ship.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Battle Tower.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Champion Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Desert.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Dewford Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Elite 4 Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Ending Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Evergrande City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Fallarbor Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Fortree City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Heavy Rain.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Lilycove City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Lilycove Museum.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Littleroot Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Mt Pyre Peak.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Mt Pyre.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Oceanic Museum.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Oldale Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Petalburg City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Pokemart.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Pokemon Contest.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Recovery.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Rival Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Route 101.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Route 104.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Route 110.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Route 111 Desert.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Route 120.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Slateport City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Sootopolis City.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Surfing.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Trainers School.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Trick House.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Verdanturf Town.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Wild Pokemon Battle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon RubySapphireEmerald - Wild Pokemon Caught or Fainted.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Snap - A Mysterious Sighting.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Snap - Snapshot Lullaby.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Trading Card Game - Title Screen.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon X and Y - Calem and Serena.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XD Gale of Darkness - Mt Battle Lobby.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XD Gale of Darkness - The Hexagon Bros.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - Battle Wild Pokemon.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - Bicycle.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - Emmas Theme.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - FIN.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - Route 4567.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - The Sycamore Pokemon Lab.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon XY - Together with Shauna.mid\n",
      "Parsing Pokemon MIDIs\\Pokemon Yellow - Lavender Town (Fixed).mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda3\\envs\\CITS4404\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.697306, acc.: 0.00%] [G loss: 0.690526]\n",
      "1 [D loss: 0.678759, acc.: 70.31%] [G loss: 0.690840]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda3\\envs\\CITS4404\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [D loss: 0.650819, acc.: 73.44%] [G loss: 0.700503]\n",
      "3 [D loss: 0.608170, acc.: 68.75%] [G loss: 0.734438]\n",
      "4 [D loss: 0.444126, acc.: 81.25%] [G loss: 1.164595]\n",
      "5 [D loss: 0.305119, acc.: 92.19%] [G loss: 1.650650]\n",
      "6 [D loss: 0.342359, acc.: 92.19%] [G loss: 1.844017]\n",
      "7 [D loss: 0.170589, acc.: 92.19%] [G loss: 4.820179]\n",
      "8 [D loss: 0.211658, acc.: 93.75%] [G loss: 3.982712]\n",
      "9 [D loss: 0.116511, acc.: 96.88%] [G loss: 3.913697]\n",
      "10 [D loss: 0.155016, acc.: 90.62%] [G loss: 4.295865]\n",
      "11 [D loss: 0.095830, acc.: 95.31%] [G loss: 3.836005]\n",
      "12 [D loss: 0.605480, acc.: 78.12%] [G loss: 1.214869]\n",
      "13 [D loss: 0.327617, acc.: 92.19%] [G loss: 1.301511]\n",
      "14 [D loss: 0.333180, acc.: 89.06%] [G loss: 1.708986]\n",
      "15 [D loss: 0.239297, acc.: 89.06%] [G loss: 2.090464]\n",
      "16 [D loss: 0.435803, acc.: 82.81%] [G loss: 1.715339]\n",
      "17 [D loss: 0.332776, acc.: 82.81%] [G loss: 1.929864]\n",
      "18 [D loss: 0.352504, acc.: 87.50%] [G loss: 1.945297]\n",
      "19 [D loss: 0.312506, acc.: 90.62%] [G loss: 2.219107]\n",
      "20 [D loss: 0.340639, acc.: 89.06%] [G loss: 1.972291]\n",
      "21 [D loss: 0.273403, acc.: 87.50%] [G loss: 1.915807]\n",
      "22 [D loss: 0.489125, acc.: 78.12%] [G loss: 2.063094]\n",
      "23 [D loss: 0.253838, acc.: 90.62%] [G loss: 2.018396]\n",
      "24 [D loss: 0.338320, acc.: 87.50%] [G loss: 1.881268]\n",
      "25 [D loss: 0.402374, acc.: 84.38%] [G loss: 1.852467]\n",
      "26 [D loss: 0.243273, acc.: 89.06%] [G loss: 2.591365]\n",
      "27 [D loss: 0.205961, acc.: 92.19%] [G loss: 2.699844]\n",
      "28 [D loss: 0.431901, acc.: 78.12%] [G loss: 1.839601]\n",
      "29 [D loss: 0.420432, acc.: 84.38%] [G loss: 1.574165]\n",
      "30 [D loss: 0.298989, acc.: 92.19%] [G loss: 1.914860]\n",
      "31 [D loss: 0.396240, acc.: 89.06%] [G loss: 1.933654]\n",
      "32 [D loss: 0.316700, acc.: 85.94%] [G loss: 1.927280]\n",
      "33 [D loss: 0.406617, acc.: 85.94%] [G loss: 1.777955]\n",
      "34 [D loss: 0.301266, acc.: 85.94%] [G loss: 1.817448]\n",
      "35 [D loss: 0.318516, acc.: 87.50%] [G loss: 2.030834]\n",
      "36 [D loss: 0.375173, acc.: 87.50%] [G loss: 2.097679]\n",
      "37 [D loss: 0.398414, acc.: 87.50%] [G loss: 1.889977]\n",
      "38 [D loss: 0.309424, acc.: 90.62%] [G loss: 2.007638]\n",
      "39 [D loss: 0.289793, acc.: 89.06%] [G loss: 2.270139]\n",
      "40 [D loss: 0.206994, acc.: 93.75%] [G loss: 2.400679]\n",
      "41 [D loss: 0.371464, acc.: 87.50%] [G loss: 2.235308]\n",
      "42 [D loss: 0.328372, acc.: 87.50%] [G loss: 2.169736]\n",
      "43 [D loss: 0.204393, acc.: 93.75%] [G loss: 2.389920]\n",
      "44 [D loss: 0.295318, acc.: 90.62%] [G loss: 2.766954]\n",
      "45 [D loss: 0.327907, acc.: 85.94%] [G loss: 2.611900]\n",
      "46 [D loss: 0.409487, acc.: 82.81%] [G loss: 2.282267]\n",
      "47 [D loss: 0.354711, acc.: 87.50%] [G loss: 2.050577]\n",
      "48 [D loss: 0.465897, acc.: 79.69%] [G loss: 1.690621]\n",
      "49 [D loss: 0.341931, acc.: 87.50%] [G loss: 1.798672]\n",
      "50 [D loss: 0.445638, acc.: 79.69%] [G loss: 1.936262]\n",
      "51 [D loss: 0.361326, acc.: 84.38%] [G loss: 2.106204]\n",
      "52 [D loss: 0.480958, acc.: 82.81%] [G loss: 1.598679]\n",
      "53 [D loss: 0.373891, acc.: 84.38%] [G loss: 1.674369]\n",
      "54 [D loss: 0.401196, acc.: 78.12%] [G loss: 1.708685]\n",
      "55 [D loss: 0.343207, acc.: 87.50%] [G loss: 1.907434]\n",
      "56 [D loss: 0.277910, acc.: 89.06%] [G loss: 2.241952]\n",
      "57 [D loss: 0.519484, acc.: 82.81%] [G loss: 1.641328]\n",
      "58 [D loss: 0.437975, acc.: 84.38%] [G loss: 1.501012]\n",
      "59 [D loss: 0.444612, acc.: 82.81%] [G loss: 1.469619]\n",
      "60 [D loss: 0.305711, acc.: 89.06%] [G loss: 1.510876]\n",
      "61 [D loss: 0.332196, acc.: 89.06%] [G loss: 1.649235]\n",
      "62 [D loss: 0.285366, acc.: 89.06%] [G loss: 1.973712]\n",
      "63 [D loss: 0.245350, acc.: 89.06%] [G loss: 2.075163]\n",
      "64 [D loss: 0.361104, acc.: 89.06%] [G loss: 2.015610]\n",
      "65 [D loss: 0.352271, acc.: 89.06%] [G loss: 1.800266]\n",
      "66 [D loss: 0.303638, acc.: 90.62%] [G loss: 1.885667]\n",
      "67 [D loss: 0.328025, acc.: 81.25%] [G loss: 1.855632]\n",
      "68 [D loss: 0.377804, acc.: 82.81%] [G loss: 1.852491]\n",
      "69 [D loss: 0.297933, acc.: 85.94%] [G loss: 2.424638]\n",
      "70 [D loss: 0.372047, acc.: 82.81%] [G loss: 1.824374]\n",
      "71 [D loss: 0.382705, acc.: 81.25%] [G loss: 1.762549]\n",
      "72 [D loss: 0.312429, acc.: 85.94%] [G loss: 2.044508]\n",
      "73 [D loss: 0.489887, acc.: 78.12%] [G loss: 1.714108]\n",
      "74 [D loss: 0.331733, acc.: 87.50%] [G loss: 1.845649]\n",
      "75 [D loss: 0.466261, acc.: 84.38%] [G loss: 1.659826]\n",
      "76 [D loss: 0.588254, acc.: 70.31%] [G loss: 1.497038]\n",
      "77 [D loss: 0.389389, acc.: 81.25%] [G loss: 1.660462]\n",
      "78 [D loss: 0.358779, acc.: 82.81%] [G loss: 1.990692]\n",
      "79 [D loss: 0.262182, acc.: 87.50%] [G loss: 2.346297]\n",
      "80 [D loss: 0.321966, acc.: 85.94%] [G loss: 2.482038]\n",
      "81 [D loss: 0.535154, acc.: 84.38%] [G loss: 1.530332]\n",
      "82 [D loss: 0.470809, acc.: 82.81%] [G loss: 1.431655]\n",
      "83 [D loss: 0.304871, acc.: 92.19%] [G loss: 1.609613]\n",
      "84 [D loss: 0.518658, acc.: 81.25%] [G loss: 1.468227]\n",
      "85 [D loss: 0.456272, acc.: 78.12%] [G loss: 1.378179]\n",
      "86 [D loss: 0.385550, acc.: 85.94%] [G loss: 1.513136]\n",
      "87 [D loss: 0.361430, acc.: 89.06%] [G loss: 1.643059]\n",
      "88 [D loss: 0.392038, acc.: 82.81%] [G loss: 1.916706]\n",
      "89 [D loss: 0.407316, acc.: 84.38%] [G loss: 1.818201]\n",
      "90 [D loss: 0.371750, acc.: 87.50%] [G loss: 1.776494]\n",
      "91 [D loss: 0.550335, acc.: 76.56%] [G loss: 1.507431]\n",
      "92 [D loss: 0.473926, acc.: 78.12%] [G loss: 1.346667]\n",
      "93 [D loss: 0.405792, acc.: 82.81%] [G loss: 1.441667]\n",
      "94 [D loss: 0.517648, acc.: 81.25%] [G loss: 1.482560]\n",
      "95 [D loss: 0.400873, acc.: 81.25%] [G loss: 1.556723]\n",
      "96 [D loss: 0.460154, acc.: 82.81%] [G loss: 1.595266]\n",
      "97 [D loss: 0.424791, acc.: 79.69%] [G loss: 1.765153]\n",
      "98 [D loss: 0.612212, acc.: 68.75%] [G loss: 1.534538]\n",
      "99 [D loss: 0.409322, acc.: 84.38%] [G loss: 1.573132]\n",
      "100 [D loss: 0.657525, acc.: 67.19%] [G loss: 1.356835]\n",
      "101 [D loss: 0.488421, acc.: 76.56%] [G loss: 1.364800]\n",
      "102 [D loss: 0.494733, acc.: 71.88%] [G loss: 1.426594]\n",
      "103 [D loss: 0.431901, acc.: 84.38%] [G loss: 1.568926]\n",
      "104 [D loss: 0.521598, acc.: 76.56%] [G loss: 1.489117]\n",
      "105 [D loss: 0.482670, acc.: 75.00%] [G loss: 1.385990]\n",
      "106 [D loss: 0.440365, acc.: 85.94%] [G loss: 1.438380]\n",
      "107 [D loss: 0.365762, acc.: 87.50%] [G loss: 1.673265]\n",
      "108 [D loss: 0.773507, acc.: 64.06%] [G loss: 1.354316]\n",
      "109 [D loss: 0.478142, acc.: 76.56%] [G loss: 1.202963]\n",
      "110 [D loss: 0.465282, acc.: 81.25%] [G loss: 1.339357]\n",
      "111 [D loss: 0.463691, acc.: 78.12%] [G loss: 1.419198]\n",
      "112 [D loss: 0.515281, acc.: 76.56%] [G loss: 1.573048]\n",
      "113 [D loss: 0.290962, acc.: 87.50%] [G loss: 1.833107]\n",
      "114 [D loss: 0.571668, acc.: 67.19%] [G loss: 1.775625]\n",
      "115 [D loss: 0.498289, acc.: 75.00%] [G loss: 1.542515]\n",
      "116 [D loss: 0.536567, acc.: 78.12%] [G loss: 1.553976]\n",
      "117 [D loss: 0.513813, acc.: 71.88%] [G loss: 1.516224]\n",
      "118 [D loss: 0.447662, acc.: 81.25%] [G loss: 1.579061]\n",
      "119 [D loss: 0.484449, acc.: 79.69%] [G loss: 1.878972]\n",
      "120 [D loss: 0.654496, acc.: 70.31%] [G loss: 1.512900]\n",
      "121 [D loss: 0.561904, acc.: 70.31%] [G loss: 1.379697]\n",
      "122 [D loss: 0.432071, acc.: 81.25%] [G loss: 1.581122]\n",
      "123 [D loss: 0.500218, acc.: 76.56%] [G loss: 1.528685]\n",
      "124 [D loss: 0.471163, acc.: 73.44%] [G loss: 1.633283]\n",
      "125 [D loss: 0.571860, acc.: 68.75%] [G loss: 1.577049]\n",
      "126 [D loss: 0.604735, acc.: 71.88%] [G loss: 1.211348]\n",
      "127 [D loss: 0.502600, acc.: 76.56%] [G loss: 1.313595]\n",
      "128 [D loss: 0.502462, acc.: 75.00%] [G loss: 1.436796]\n",
      "129 [D loss: 0.553314, acc.: 75.00%] [G loss: 1.450648]\n",
      "130 [D loss: 0.575831, acc.: 68.75%] [G loss: 1.228946]\n",
      "131 [D loss: 0.527880, acc.: 78.12%] [G loss: 1.241436]\n",
      "132 [D loss: 0.441719, acc.: 81.25%] [G loss: 1.484190]\n",
      "133 [D loss: 0.466516, acc.: 84.38%] [G loss: 1.673629]\n",
      "134 [D loss: 0.400319, acc.: 81.25%] [G loss: 1.789393]\n",
      "135 [D loss: 0.607922, acc.: 68.75%] [G loss: 1.428159]\n",
      "136 [D loss: 0.437814, acc.: 78.12%] [G loss: 1.482264]\n",
      "137 [D loss: 0.550419, acc.: 71.88%] [G loss: 1.536266]\n",
      "138 [D loss: 0.647417, acc.: 70.31%] [G loss: 1.340825]\n",
      "139 [D loss: 0.493535, acc.: 82.81%] [G loss: 1.307514]\n",
      "140 [D loss: 0.558331, acc.: 71.88%] [G loss: 1.369155]\n",
      "141 [D loss: 0.644855, acc.: 65.62%] [G loss: 1.190661]\n",
      "142 [D loss: 0.520638, acc.: 76.56%] [G loss: 1.288509]\n",
      "143 [D loss: 0.620083, acc.: 68.75%] [G loss: 1.243889]\n",
      "144 [D loss: 0.423282, acc.: 84.38%] [G loss: 1.469890]\n",
      "145 [D loss: 0.612503, acc.: 68.75%] [G loss: 1.439201]\n",
      "146 [D loss: 0.543205, acc.: 78.12%] [G loss: 1.421385]\n",
      "147 [D loss: 0.505674, acc.: 75.00%] [G loss: 1.450246]\n",
      "148 [D loss: 0.468337, acc.: 73.44%] [G loss: 1.758024]\n",
      "149 [D loss: 0.581737, acc.: 65.62%] [G loss: 1.561810]\n",
      "150 [D loss: 0.507500, acc.: 75.00%] [G loss: 1.457850]\n",
      "151 [D loss: 0.502563, acc.: 76.56%] [G loss: 1.538685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 [D loss: 0.407307, acc.: 79.69%] [G loss: 1.816365]\n",
      "153 [D loss: 0.543781, acc.: 76.56%] [G loss: 1.554956]\n",
      "154 [D loss: 0.484053, acc.: 73.44%] [G loss: 1.597312]\n",
      "155 [D loss: 0.480821, acc.: 81.25%] [G loss: 1.583509]\n",
      "156 [D loss: 0.424812, acc.: 81.25%] [G loss: 2.114580]\n",
      "157 [D loss: 0.424694, acc.: 82.81%] [G loss: 2.025404]\n",
      "158 [D loss: 0.356315, acc.: 81.25%] [G loss: 1.797617]\n",
      "159 [D loss: 0.626920, acc.: 68.75%] [G loss: 1.249624]\n",
      "160 [D loss: 0.528003, acc.: 81.25%] [G loss: 1.241635]\n",
      "161 [D loss: 0.495531, acc.: 79.69%] [G loss: 1.366838]\n",
      "162 [D loss: 0.508053, acc.: 79.69%] [G loss: 1.502638]\n",
      "163 [D loss: 0.457349, acc.: 76.56%] [G loss: 1.689928]\n",
      "164 [D loss: 0.503897, acc.: 76.56%] [G loss: 1.548302]\n",
      "165 [D loss: 0.518112, acc.: 78.12%] [G loss: 1.616982]\n",
      "166 [D loss: 0.483374, acc.: 79.69%] [G loss: 1.641395]\n",
      "167 [D loss: 0.531240, acc.: 78.12%] [G loss: 1.365087]\n",
      "168 [D loss: 0.545414, acc.: 71.88%] [G loss: 1.379583]\n",
      "169 [D loss: 0.588961, acc.: 68.75%] [G loss: 1.281625]\n",
      "170 [D loss: 0.573144, acc.: 71.88%] [G loss: 1.376117]\n",
      "171 [D loss: 0.721924, acc.: 65.62%] [G loss: 1.081066]\n",
      "172 [D loss: 0.502493, acc.: 76.56%] [G loss: 1.166121]\n",
      "173 [D loss: 0.579400, acc.: 78.12%] [G loss: 1.168575]\n",
      "174 [D loss: 0.518403, acc.: 71.88%] [G loss: 1.346129]\n",
      "175 [D loss: 0.607322, acc.: 68.75%] [G loss: 1.350753]\n",
      "176 [D loss: 0.458457, acc.: 79.69%] [G loss: 1.663948]\n",
      "177 [D loss: 0.556639, acc.: 71.88%] [G loss: 1.483872]\n",
      "178 [D loss: 0.592161, acc.: 71.88%] [G loss: 1.242350]\n",
      "179 [D loss: 0.675956, acc.: 67.19%] [G loss: 1.118241]\n",
      "180 [D loss: 0.618068, acc.: 62.50%] [G loss: 1.101938]\n",
      "181 [D loss: 0.690115, acc.: 67.19%] [G loss: 0.991783]\n",
      "182 [D loss: 0.602345, acc.: 70.31%] [G loss: 0.983579]\n",
      "183 [D loss: 0.611650, acc.: 67.19%] [G loss: 1.005767]\n",
      "184 [D loss: 0.565086, acc.: 81.25%] [G loss: 1.067188]\n",
      "185 [D loss: 0.607908, acc.: 62.50%] [G loss: 1.210659]\n",
      "186 [D loss: 0.561476, acc.: 71.88%] [G loss: 1.209924]\n",
      "187 [D loss: 0.562890, acc.: 68.75%] [G loss: 1.214515]\n",
      "188 [D loss: 0.708704, acc.: 60.94%] [G loss: 1.168219]\n",
      "189 [D loss: 0.526159, acc.: 73.44%] [G loss: 1.248333]\n",
      "190 [D loss: 0.570063, acc.: 70.31%] [G loss: 1.301199]\n",
      "191 [D loss: 0.660960, acc.: 60.94%] [G loss: 1.179469]\n",
      "192 [D loss: 0.711978, acc.: 60.94%] [G loss: 0.975250]\n",
      "193 [D loss: 0.558725, acc.: 70.31%] [G loss: 1.025040]\n",
      "194 [D loss: 0.656121, acc.: 64.06%] [G loss: 1.053918]\n",
      "195 [D loss: 0.499289, acc.: 84.38%] [G loss: 1.138466]\n",
      "196 [D loss: 0.550212, acc.: 78.12%] [G loss: 1.110119]\n",
      "197 [D loss: 0.527371, acc.: 75.00%] [G loss: 1.198982]\n",
      "198 [D loss: 0.485336, acc.: 76.56%] [G loss: 1.202956]\n",
      "199 [D loss: 0.672996, acc.: 65.62%] [G loss: 1.176120]\n",
      "200 [D loss: 0.658871, acc.: 65.62%] [G loss: 1.024694]\n",
      "201 [D loss: 0.606929, acc.: 67.19%] [G loss: 1.034812]\n",
      "202 [D loss: 0.574820, acc.: 70.31%] [G loss: 1.028249]\n",
      "203 [D loss: 0.592892, acc.: 68.75%] [G loss: 1.007142]\n",
      "204 [D loss: 0.580424, acc.: 68.75%] [G loss: 1.043498]\n",
      "205 [D loss: 0.529098, acc.: 70.31%] [G loss: 1.077784]\n",
      "206 [D loss: 0.627246, acc.: 62.50%] [G loss: 0.990457]\n",
      "207 [D loss: 0.528853, acc.: 73.44%] [G loss: 1.048524]\n",
      "208 [D loss: 0.546876, acc.: 70.31%] [G loss: 1.136700]\n",
      "209 [D loss: 0.527997, acc.: 71.88%] [G loss: 1.137733]\n",
      "210 [D loss: 0.621591, acc.: 65.62%] [G loss: 1.173624]\n",
      "211 [D loss: 0.572860, acc.: 78.12%] [G loss: 1.074209]\n",
      "212 [D loss: 0.643776, acc.: 62.50%] [G loss: 1.018934]\n",
      "213 [D loss: 0.526404, acc.: 73.44%] [G loss: 1.081824]\n",
      "214 [D loss: 0.518657, acc.: 76.56%] [G loss: 1.093985]\n",
      "215 [D loss: 0.746389, acc.: 57.81%] [G loss: 0.972204]\n",
      "216 [D loss: 0.571707, acc.: 73.44%] [G loss: 0.964530]\n",
      "217 [D loss: 0.640143, acc.: 60.94%] [G loss: 0.923256]\n",
      "218 [D loss: 0.572830, acc.: 71.88%] [G loss: 0.946528]\n",
      "219 [D loss: 0.665419, acc.: 64.06%] [G loss: 0.916015]\n",
      "220 [D loss: 0.565913, acc.: 70.31%] [G loss: 0.927738]\n",
      "221 [D loss: 0.642162, acc.: 64.06%] [G loss: 0.939287]\n",
      "222 [D loss: 0.624199, acc.: 70.31%] [G loss: 0.923220]\n",
      "223 [D loss: 0.559050, acc.: 67.19%] [G loss: 0.926618]\n",
      "224 [D loss: 0.653613, acc.: 56.25%] [G loss: 0.927238]\n",
      "225 [D loss: 0.643589, acc.: 65.62%] [G loss: 0.903113]\n",
      "226 [D loss: 0.743767, acc.: 56.25%] [G loss: 0.881867]\n",
      "227 [D loss: 0.565150, acc.: 71.88%] [G loss: 0.896667]\n",
      "228 [D loss: 0.658245, acc.: 60.94%] [G loss: 0.895315]\n",
      "229 [D loss: 0.630149, acc.: 64.06%] [G loss: 0.893343]\n",
      "230 [D loss: 0.586317, acc.: 68.75%] [G loss: 0.888476]\n",
      "231 [D loss: 0.583772, acc.: 68.75%] [G loss: 0.931105]\n",
      "232 [D loss: 0.542662, acc.: 73.44%] [G loss: 0.966046]\n",
      "233 [D loss: 0.581124, acc.: 70.31%] [G loss: 0.996248]\n",
      "234 [D loss: 0.712806, acc.: 53.12%] [G loss: 0.882106]\n",
      "235 [D loss: 0.620371, acc.: 64.06%] [G loss: 0.837774]\n",
      "236 [D loss: 0.775692, acc.: 45.31%] [G loss: 0.810744]\n",
      "237 [D loss: 0.593700, acc.: 71.88%] [G loss: 0.827895]\n",
      "238 [D loss: 0.659481, acc.: 60.94%] [G loss: 0.837454]\n",
      "239 [D loss: 0.694878, acc.: 54.69%] [G loss: 0.806007]\n",
      "240 [D loss: 0.661508, acc.: 60.94%] [G loss: 0.811937]\n",
      "241 [D loss: 0.606439, acc.: 75.00%] [G loss: 0.838518]\n",
      "242 [D loss: 0.700398, acc.: 54.69%] [G loss: 0.803653]\n",
      "243 [D loss: 0.612931, acc.: 68.75%] [G loss: 0.819899]\n",
      "244 [D loss: 0.603531, acc.: 70.31%] [G loss: 0.831731]\n",
      "245 [D loss: 0.659864, acc.: 64.06%] [G loss: 0.818190]\n",
      "246 [D loss: 0.636820, acc.: 71.88%] [G loss: 0.840635]\n",
      "247 [D loss: 0.683802, acc.: 57.81%] [G loss: 0.821696]\n",
      "248 [D loss: 0.675750, acc.: 59.38%] [G loss: 0.825125]\n",
      "249 [D loss: 0.687163, acc.: 60.94%] [G loss: 0.821943]\n",
      "250 [D loss: 0.708671, acc.: 53.12%] [G loss: 0.818424]\n",
      "251 [D loss: 0.693412, acc.: 54.69%] [G loss: 0.811963]\n",
      "252 [D loss: 0.654485, acc.: 62.50%] [G loss: 0.814522]\n",
      "253 [D loss: 0.649592, acc.: 59.38%] [G loss: 0.805892]\n",
      "254 [D loss: 0.653726, acc.: 60.94%] [G loss: 0.820592]\n",
      "255 [D loss: 0.710746, acc.: 51.56%] [G loss: 0.794622]\n",
      "256 [D loss: 0.662023, acc.: 67.19%] [G loss: 0.791553]\n",
      "257 [D loss: 0.716180, acc.: 50.00%] [G loss: 0.780471]\n",
      "258 [D loss: 0.669946, acc.: 57.81%] [G loss: 0.783750]\n",
      "259 [D loss: 0.660385, acc.: 56.25%] [G loss: 0.769001]\n",
      "260 [D loss: 0.703378, acc.: 56.25%] [G loss: 0.776225]\n",
      "261 [D loss: 0.677246, acc.: 60.94%] [G loss: 0.768237]\n",
      "262 [D loss: 0.695406, acc.: 53.12%] [G loss: 0.777096]\n",
      "263 [D loss: 0.709176, acc.: 50.00%] [G loss: 0.766287]\n",
      "264 [D loss: 0.665433, acc.: 64.06%] [G loss: 0.753806]\n",
      "265 [D loss: 0.663793, acc.: 62.50%] [G loss: 0.753953]\n",
      "266 [D loss: 0.663125, acc.: 59.38%] [G loss: 0.770533]\n",
      "267 [D loss: 0.691049, acc.: 54.69%] [G loss: 0.735733]\n",
      "268 [D loss: 0.669393, acc.: 57.81%] [G loss: 0.754062]\n",
      "269 [D loss: 0.669051, acc.: 62.50%] [G loss: 0.773516]\n",
      "270 [D loss: 0.705209, acc.: 60.94%] [G loss: 0.740206]\n",
      "271 [D loss: 0.718019, acc.: 53.12%] [G loss: 0.734077]\n",
      "272 [D loss: 0.662942, acc.: 60.94%] [G loss: 0.736387]\n",
      "273 [D loss: 0.662218, acc.: 62.50%] [G loss: 0.745262]\n",
      "274 [D loss: 0.732031, acc.: 45.31%] [G loss: 0.749736]\n",
      "275 [D loss: 0.653255, acc.: 60.94%] [G loss: 0.756215]\n",
      "276 [D loss: 0.686782, acc.: 59.38%] [G loss: 0.737784]\n",
      "277 [D loss: 0.674805, acc.: 56.25%] [G loss: 0.738143]\n",
      "278 [D loss: 0.685385, acc.: 54.69%] [G loss: 0.751421]\n",
      "279 [D loss: 0.659364, acc.: 57.81%] [G loss: 0.757937]\n",
      "280 [D loss: 0.713913, acc.: 54.69%] [G loss: 0.743355]\n",
      "281 [D loss: 0.744263, acc.: 48.44%] [G loss: 0.729799]\n",
      "282 [D loss: 0.722594, acc.: 51.56%] [G loss: 0.712355]\n",
      "283 [D loss: 0.684283, acc.: 56.25%] [G loss: 0.713700]\n",
      "284 [D loss: 0.703207, acc.: 53.12%] [G loss: 0.712713]\n",
      "285 [D loss: 0.698395, acc.: 56.25%] [G loss: 0.717153]\n",
      "286 [D loss: 0.688991, acc.: 56.25%] [G loss: 0.712492]\n",
      "287 [D loss: 0.710553, acc.: 51.56%] [G loss: 0.705459]\n",
      "288 [D loss: 0.696863, acc.: 51.56%] [G loss: 0.710734]\n",
      "289 [D loss: 0.696953, acc.: 53.12%] [G loss: 0.712012]\n",
      "290 [D loss: 0.696975, acc.: 56.25%] [G loss: 0.705789]\n",
      "291 [D loss: 0.692387, acc.: 51.56%] [G loss: 0.703080]\n",
      "292 [D loss: 0.694230, acc.: 50.00%] [G loss: 0.713306]\n",
      "293 [D loss: 0.683793, acc.: 59.38%] [G loss: 0.708811]\n",
      "294 [D loss: 0.702359, acc.: 46.88%] [G loss: 0.708539]\n",
      "295 [D loss: 0.681081, acc.: 56.25%] [G loss: 0.708472]\n",
      "296 [D loss: 0.707696, acc.: 43.75%] [G loss: 0.708594]\n",
      "297 [D loss: 0.700028, acc.: 48.44%] [G loss: 0.704782]\n",
      "298 [D loss: 0.682229, acc.: 53.12%] [G loss: 0.706467]\n",
      "299 [D loss: 0.680344, acc.: 59.38%] [G loss: 0.703264]\n",
      "300 [D loss: 0.723435, acc.: 32.81%] [G loss: 0.705872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 [D loss: 0.684163, acc.: 56.25%] [G loss: 0.709775]\n",
      "302 [D loss: 0.677682, acc.: 62.50%] [G loss: 0.708760]\n",
      "303 [D loss: 0.705878, acc.: 48.44%] [G loss: 0.702957]\n",
      "304 [D loss: 0.677360, acc.: 60.94%] [G loss: 0.705285]\n",
      "305 [D loss: 0.675259, acc.: 59.38%] [G loss: 0.704273]\n",
      "306 [D loss: 0.701075, acc.: 50.00%] [G loss: 0.706806]\n",
      "307 [D loss: 0.705814, acc.: 51.56%] [G loss: 0.702416]\n",
      "308 [D loss: 0.670498, acc.: 60.94%] [G loss: 0.702546]\n",
      "309 [D loss: 0.698523, acc.: 43.75%] [G loss: 0.707081]\n",
      "310 [D loss: 0.690883, acc.: 48.44%] [G loss: 0.697638]\n",
      "311 [D loss: 0.693458, acc.: 48.44%] [G loss: 0.716147]\n",
      "312 [D loss: 0.684393, acc.: 54.69%] [G loss: 0.707771]\n",
      "313 [D loss: 0.678409, acc.: 57.81%] [G loss: 0.701038]\n",
      "314 [D loss: 0.701087, acc.: 46.88%] [G loss: 0.714053]\n",
      "315 [D loss: 0.691648, acc.: 50.00%] [G loss: 0.710550]\n",
      "316 [D loss: 0.692114, acc.: 51.56%] [G loss: 0.702774]\n",
      "317 [D loss: 0.693039, acc.: 53.12%] [G loss: 0.703363]\n",
      "318 [D loss: 0.694629, acc.: 51.56%] [G loss: 0.705003]\n",
      "319 [D loss: 0.689722, acc.: 51.56%] [G loss: 0.705966]\n",
      "320 [D loss: 0.707938, acc.: 51.56%] [G loss: 0.694985]\n",
      "321 [D loss: 0.700268, acc.: 43.75%] [G loss: 0.714815]\n",
      "322 [D loss: 0.702757, acc.: 45.31%] [G loss: 0.706216]\n",
      "323 [D loss: 0.705305, acc.: 39.06%] [G loss: 0.717541]\n",
      "324 [D loss: 0.682806, acc.: 57.81%] [G loss: 0.706447]\n",
      "325 [D loss: 0.686086, acc.: 51.56%] [G loss: 0.707905]\n",
      "326 [D loss: 0.684029, acc.: 51.56%] [G loss: 0.708879]\n",
      "327 [D loss: 0.670145, acc.: 60.94%] [G loss: 0.704195]\n",
      "328 [D loss: 0.684520, acc.: 53.12%] [G loss: 0.701786]\n",
      "329 [D loss: 0.698477, acc.: 53.12%] [G loss: 0.708123]\n",
      "330 [D loss: 0.690717, acc.: 48.44%] [G loss: 0.707195]\n",
      "331 [D loss: 0.673532, acc.: 59.38%] [G loss: 0.702811]\n",
      "332 [D loss: 0.698902, acc.: 51.56%] [G loss: 0.702302]\n",
      "333 [D loss: 0.685463, acc.: 56.25%] [G loss: 0.707167]\n",
      "334 [D loss: 0.690683, acc.: 50.00%] [G loss: 0.701187]\n",
      "335 [D loss: 0.688973, acc.: 50.00%] [G loss: 0.700755]\n",
      "336 [D loss: 0.692344, acc.: 57.81%] [G loss: 0.702656]\n",
      "337 [D loss: 0.694923, acc.: 46.88%] [G loss: 0.703592]\n",
      "338 [D loss: 0.690103, acc.: 57.81%] [G loss: 0.698935]\n",
      "339 [D loss: 0.699044, acc.: 46.88%] [G loss: 0.699764]\n",
      "340 [D loss: 0.699464, acc.: 46.88%] [G loss: 0.701027]\n",
      "341 [D loss: 0.684072, acc.: 62.50%] [G loss: 0.701776]\n",
      "342 [D loss: 0.689426, acc.: 42.19%] [G loss: 0.706877]\n",
      "343 [D loss: 0.693537, acc.: 51.56%] [G loss: 0.699242]\n",
      "344 [D loss: 0.691298, acc.: 50.00%] [G loss: 0.700232]\n",
      "345 [D loss: 0.710927, acc.: 42.19%] [G loss: 0.695988]\n",
      "346 [D loss: 0.698754, acc.: 50.00%] [G loss: 0.696939]\n",
      "347 [D loss: 0.700857, acc.: 43.75%] [G loss: 0.696538]\n",
      "348 [D loss: 0.711237, acc.: 42.19%] [G loss: 0.704752]\n",
      "349 [D loss: 0.691546, acc.: 53.12%] [G loss: 0.697996]\n",
      "350 [D loss: 0.711057, acc.: 42.19%] [G loss: 0.698686]\n",
      "351 [D loss: 0.701856, acc.: 48.44%] [G loss: 0.707339]\n",
      "352 [D loss: 0.701433, acc.: 50.00%] [G loss: 0.704091]\n",
      "353 [D loss: 0.693782, acc.: 43.75%] [G loss: 0.703283]\n",
      "354 [D loss: 0.693087, acc.: 54.69%] [G loss: 0.696751]\n",
      "355 [D loss: 0.683542, acc.: 56.25%] [G loss: 0.699492]\n",
      "356 [D loss: 0.709440, acc.: 37.50%] [G loss: 0.696535]\n",
      "357 [D loss: 0.706721, acc.: 42.19%] [G loss: 0.694642]\n",
      "358 [D loss: 0.693095, acc.: 51.56%] [G loss: 0.701973]\n",
      "359 [D loss: 0.698710, acc.: 43.75%] [G loss: 0.693660]\n",
      "360 [D loss: 0.692850, acc.: 54.69%] [G loss: 0.697845]\n",
      "361 [D loss: 0.691562, acc.: 51.56%] [G loss: 0.692230]\n",
      "362 [D loss: 0.702348, acc.: 35.94%] [G loss: 0.697702]\n",
      "363 [D loss: 0.695396, acc.: 43.75%] [G loss: 0.698635]\n",
      "364 [D loss: 0.689223, acc.: 54.69%] [G loss: 0.703762]\n",
      "365 [D loss: 0.707970, acc.: 45.31%] [G loss: 0.697654]\n",
      "366 [D loss: 0.697851, acc.: 42.19%] [G loss: 0.696599]\n",
      "367 [D loss: 0.698517, acc.: 40.62%] [G loss: 0.706879]\n",
      "368 [D loss: 0.700688, acc.: 40.62%] [G loss: 0.695867]\n",
      "369 [D loss: 0.686108, acc.: 59.38%] [G loss: 0.701327]\n",
      "370 [D loss: 0.693368, acc.: 43.75%] [G loss: 0.703916]\n",
      "371 [D loss: 0.697617, acc.: 45.31%] [G loss: 0.701362]\n",
      "372 [D loss: 0.700983, acc.: 45.31%] [G loss: 0.691667]\n",
      "373 [D loss: 0.702406, acc.: 50.00%] [G loss: 0.701592]\n",
      "374 [D loss: 0.700091, acc.: 46.88%] [G loss: 0.700092]\n",
      "375 [D loss: 0.693955, acc.: 51.56%] [G loss: 0.704248]\n",
      "376 [D loss: 0.692219, acc.: 46.88%] [G loss: 0.696384]\n",
      "377 [D loss: 0.696368, acc.: 50.00%] [G loss: 0.707933]\n",
      "378 [D loss: 0.698129, acc.: 42.19%] [G loss: 0.701699]\n",
      "379 [D loss: 0.698292, acc.: 45.31%] [G loss: 0.705158]\n",
      "380 [D loss: 0.694189, acc.: 45.31%] [G loss: 0.699239]\n",
      "381 [D loss: 0.700495, acc.: 40.62%] [G loss: 0.706511]\n",
      "382 [D loss: 0.701690, acc.: 42.19%] [G loss: 0.706422]\n",
      "383 [D loss: 0.697371, acc.: 51.56%] [G loss: 0.703187]\n",
      "384 [D loss: 0.695743, acc.: 43.75%] [G loss: 0.707609]\n",
      "385 [D loss: 0.697304, acc.: 48.44%] [G loss: 0.705400]\n",
      "386 [D loss: 0.696243, acc.: 42.19%] [G loss: 0.707217]\n",
      "387 [D loss: 0.696945, acc.: 48.44%] [G loss: 0.706460]\n",
      "388 [D loss: 0.695503, acc.: 50.00%] [G loss: 0.709290]\n",
      "389 [D loss: 0.692195, acc.: 48.44%] [G loss: 0.709734]\n",
      "390 [D loss: 0.692428, acc.: 43.75%] [G loss: 0.702814]\n",
      "391 [D loss: 0.697136, acc.: 40.62%] [G loss: 0.705545]\n",
      "392 [D loss: 0.701733, acc.: 40.62%] [G loss: 0.708726]\n",
      "393 [D loss: 0.693383, acc.: 53.12%] [G loss: 0.711084]\n",
      "394 [D loss: 0.695865, acc.: 46.88%] [G loss: 0.714078]\n",
      "395 [D loss: 0.690456, acc.: 54.69%] [G loss: 0.709129]\n",
      "396 [D loss: 0.691474, acc.: 59.38%] [G loss: 0.702162]\n",
      "397 [D loss: 0.693464, acc.: 46.88%] [G loss: 0.709265]\n",
      "398 [D loss: 0.690252, acc.: 59.38%] [G loss: 0.710959]\n",
      "399 [D loss: 0.693439, acc.: 53.12%] [G loss: 0.715707]\n",
      "400 [D loss: 0.689576, acc.: 54.69%] [G loss: 0.714248]\n",
      "401 [D loss: 0.692733, acc.: 54.69%] [G loss: 0.711287]\n",
      "402 [D loss: 0.695847, acc.: 46.88%] [G loss: 0.711611]\n",
      "403 [D loss: 0.689611, acc.: 57.81%] [G loss: 0.709771]\n",
      "404 [D loss: 0.690630, acc.: 53.12%] [G loss: 0.718735]\n",
      "405 [D loss: 0.697782, acc.: 45.31%] [G loss: 0.709791]\n",
      "406 [D loss: 0.706191, acc.: 42.19%] [G loss: 0.709608]\n",
      "407 [D loss: 0.689378, acc.: 59.38%] [G loss: 0.712850]\n",
      "408 [D loss: 0.691338, acc.: 54.69%] [G loss: 0.711463]\n",
      "409 [D loss: 0.696102, acc.: 46.88%] [G loss: 0.717016]\n",
      "410 [D loss: 0.695880, acc.: 46.88%] [G loss: 0.720475]\n",
      "411 [D loss: 0.695990, acc.: 48.44%] [G loss: 0.711653]\n",
      "412 [D loss: 0.696308, acc.: 48.44%] [G loss: 0.716878]\n",
      "413 [D loss: 0.694706, acc.: 48.44%] [G loss: 0.717721]\n",
      "414 [D loss: 0.693893, acc.: 48.44%] [G loss: 0.719897]\n",
      "415 [D loss: 0.694560, acc.: 48.44%] [G loss: 0.713748]\n",
      "416 [D loss: 0.702404, acc.: 39.06%] [G loss: 0.711792]\n",
      "417 [D loss: 0.691776, acc.: 53.12%] [G loss: 0.716341]\n",
      "418 [D loss: 0.696975, acc.: 48.44%] [G loss: 0.710660]\n",
      "419 [D loss: 0.692917, acc.: 45.31%] [G loss: 0.714841]\n",
      "420 [D loss: 0.697169, acc.: 46.88%] [G loss: 0.712467]\n",
      "421 [D loss: 0.694977, acc.: 50.00%] [G loss: 0.714478]\n",
      "422 [D loss: 0.695681, acc.: 48.44%] [G loss: 0.712062]\n",
      "423 [D loss: 0.694985, acc.: 51.56%] [G loss: 0.711601]\n",
      "424 [D loss: 0.691611, acc.: 51.56%] [G loss: 0.714738]\n",
      "425 [D loss: 0.691190, acc.: 50.00%] [G loss: 0.712419]\n",
      "426 [D loss: 0.692194, acc.: 57.81%] [G loss: 0.705861]\n",
      "427 [D loss: 0.694629, acc.: 48.44%] [G loss: 0.709556]\n",
      "428 [D loss: 0.688915, acc.: 53.12%] [G loss: 0.712259]\n",
      "429 [D loss: 0.688058, acc.: 60.94%] [G loss: 0.707259]\n",
      "430 [D loss: 0.694074, acc.: 50.00%] [G loss: 0.707607]\n",
      "431 [D loss: 0.689190, acc.: 56.25%] [G loss: 0.702137]\n",
      "432 [D loss: 0.700212, acc.: 51.56%] [G loss: 0.706225]\n",
      "433 [D loss: 0.690367, acc.: 48.44%] [G loss: 0.700272]\n",
      "434 [D loss: 0.689563, acc.: 62.50%] [G loss: 0.708623]\n",
      "435 [D loss: 0.706352, acc.: 37.50%] [G loss: 0.706241]\n",
      "436 [D loss: 0.700990, acc.: 46.88%] [G loss: 0.700687]\n",
      "437 [D loss: 0.693280, acc.: 53.12%] [G loss: 0.704121]\n",
      "438 [D loss: 0.690163, acc.: 54.69%] [G loss: 0.708920]\n",
      "439 [D loss: 0.694273, acc.: 48.44%] [G loss: 0.703444]\n",
      "440 [D loss: 0.697005, acc.: 45.31%] [G loss: 0.699280]\n",
      "441 [D loss: 0.689585, acc.: 53.12%] [G loss: 0.700327]\n",
      "442 [D loss: 0.696712, acc.: 43.75%] [G loss: 0.700016]\n",
      "443 [D loss: 0.692717, acc.: 48.44%] [G loss: 0.706773]\n",
      "444 [D loss: 0.691463, acc.: 46.88%] [G loss: 0.709514]\n",
      "445 [D loss: 0.695644, acc.: 48.44%] [G loss: 0.706895]\n",
      "446 [D loss: 0.700501, acc.: 40.62%] [G loss: 0.706467]\n",
      "447 [D loss: 0.691768, acc.: 53.12%] [G loss: 0.704955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 [D loss: 0.692948, acc.: 46.88%] [G loss: 0.705852]\n",
      "449 [D loss: 0.699201, acc.: 43.75%] [G loss: 0.702405]\n",
      "450 [D loss: 0.699684, acc.: 34.38%] [G loss: 0.708785]\n",
      "451 [D loss: 0.696102, acc.: 53.12%] [G loss: 0.709278]\n",
      "452 [D loss: 0.689787, acc.: 54.69%] [G loss: 0.709291]\n",
      "453 [D loss: 0.693151, acc.: 48.44%] [G loss: 0.706346]\n",
      "454 [D loss: 0.694557, acc.: 45.31%] [G loss: 0.708895]\n",
      "455 [D loss: 0.695004, acc.: 43.75%] [G loss: 0.713265]\n",
      "456 [D loss: 0.693116, acc.: 54.69%] [G loss: 0.711931]\n",
      "457 [D loss: 0.693577, acc.: 50.00%] [G loss: 0.704899]\n",
      "458 [D loss: 0.696520, acc.: 51.56%] [G loss: 0.710417]\n",
      "459 [D loss: 0.694575, acc.: 45.31%] [G loss: 0.712063]\n",
      "460 [D loss: 0.691768, acc.: 45.31%] [G loss: 0.712171]\n",
      "461 [D loss: 0.694856, acc.: 42.19%] [G loss: 0.713932]\n",
      "462 [D loss: 0.698966, acc.: 43.75%] [G loss: 0.711519]\n",
      "463 [D loss: 0.692313, acc.: 48.44%] [G loss: 0.708075]\n",
      "464 [D loss: 0.693832, acc.: 45.31%] [G loss: 0.709201]\n",
      "465 [D loss: 0.695414, acc.: 50.00%] [G loss: 0.710971]\n",
      "466 [D loss: 0.700775, acc.: 48.44%] [G loss: 0.712190]\n",
      "467 [D loss: 0.695529, acc.: 50.00%] [G loss: 0.706556]\n",
      "468 [D loss: 0.691813, acc.: 51.56%] [G loss: 0.711165]\n",
      "469 [D loss: 0.696174, acc.: 46.88%] [G loss: 0.709563]\n",
      "470 [D loss: 0.693913, acc.: 53.12%] [G loss: 0.705007]\n",
      "471 [D loss: 0.695848, acc.: 45.31%] [G loss: 0.710345]\n",
      "472 [D loss: 0.693985, acc.: 48.44%] [G loss: 0.709235]\n",
      "473 [D loss: 0.692475, acc.: 48.44%] [G loss: 0.706893]\n",
      "474 [D loss: 0.694662, acc.: 39.06%] [G loss: 0.709044]\n",
      "475 [D loss: 0.695913, acc.: 42.19%] [G loss: 0.706525]\n",
      "476 [D loss: 0.691358, acc.: 51.56%] [G loss: 0.709027]\n",
      "477 [D loss: 0.694004, acc.: 46.88%] [G loss: 0.708642]\n",
      "478 [D loss: 0.692652, acc.: 54.69%] [G loss: 0.712219]\n",
      "479 [D loss: 0.694593, acc.: 51.56%] [G loss: 0.708806]\n",
      "480 [D loss: 0.692883, acc.: 54.69%] [G loss: 0.710525]\n",
      "481 [D loss: 0.690410, acc.: 53.12%] [G loss: 0.709508]\n",
      "482 [D loss: 0.696548, acc.: 42.19%] [G loss: 0.712009]\n",
      "483 [D loss: 0.689531, acc.: 53.12%] [G loss: 0.711488]\n",
      "484 [D loss: 0.695712, acc.: 53.12%] [G loss: 0.714765]\n",
      "485 [D loss: 0.694476, acc.: 51.56%] [G loss: 0.713682]\n",
      "486 [D loss: 0.697846, acc.: 50.00%] [G loss: 0.710793]\n",
      "487 [D loss: 0.696961, acc.: 46.88%] [G loss: 0.713238]\n",
      "488 [D loss: 0.696084, acc.: 46.88%] [G loss: 0.712823]\n",
      "489 [D loss: 0.691318, acc.: 57.81%] [G loss: 0.710647]\n",
      "490 [D loss: 0.695163, acc.: 51.56%] [G loss: 0.712253]\n",
      "491 [D loss: 0.693704, acc.: 51.56%] [G loss: 0.713248]\n",
      "492 [D loss: 0.692608, acc.: 54.69%] [G loss: 0.713942]\n",
      "493 [D loss: 0.694583, acc.: 50.00%] [G loss: 0.708922]\n",
      "494 [D loss: 0.694851, acc.: 56.25%] [G loss: 0.707729]\n",
      "495 [D loss: 0.692777, acc.: 54.69%] [G loss: 0.711895]\n",
      "496 [D loss: 0.692647, acc.: 50.00%] [G loss: 0.711793]\n",
      "497 [D loss: 0.693341, acc.: 51.56%] [G loss: 0.711396]\n",
      "498 [D loss: 0.690836, acc.: 48.44%] [G loss: 0.713496]\n",
      "499 [D loss: 0.690785, acc.: 54.69%] [G loss: 0.714092]\n",
      "500 [D loss: 0.692329, acc.: 51.56%] [G loss: 0.714639]\n",
      "501 [D loss: 0.693894, acc.: 54.69%] [G loss: 0.714856]\n",
      "502 [D loss: 0.693880, acc.: 50.00%] [G loss: 0.715230]\n",
      "503 [D loss: 0.696034, acc.: 48.44%] [G loss: 0.710202]\n",
      "504 [D loss: 0.692750, acc.: 53.12%] [G loss: 0.712393]\n",
      "505 [D loss: 0.693828, acc.: 56.25%] [G loss: 0.710653]\n",
      "506 [D loss: 0.694054, acc.: 54.69%] [G loss: 0.706617]\n",
      "507 [D loss: 0.695419, acc.: 46.88%] [G loss: 0.705612]\n",
      "508 [D loss: 0.690076, acc.: 54.69%] [G loss: 0.705788]\n",
      "509 [D loss: 0.689399, acc.: 50.00%] [G loss: 0.707340]\n",
      "510 [D loss: 0.690523, acc.: 53.12%] [G loss: 0.708411]\n",
      "511 [D loss: 0.694869, acc.: 42.19%] [G loss: 0.709539]\n",
      "512 [D loss: 0.696987, acc.: 45.31%] [G loss: 0.707637]\n",
      "513 [D loss: 0.697541, acc.: 40.62%] [G loss: 0.707037]\n",
      "514 [D loss: 0.695859, acc.: 48.44%] [G loss: 0.706393]\n",
      "515 [D loss: 0.696383, acc.: 57.81%] [G loss: 0.707800]\n",
      "516 [D loss: 0.693224, acc.: 46.88%] [G loss: 0.707944]\n",
      "517 [D loss: 0.694586, acc.: 46.88%] [G loss: 0.705386]\n",
      "518 [D loss: 0.697959, acc.: 53.12%] [G loss: 0.709132]\n",
      "519 [D loss: 0.694122, acc.: 43.75%] [G loss: 0.708891]\n",
      "520 [D loss: 0.694476, acc.: 54.69%] [G loss: 0.707924]\n",
      "521 [D loss: 0.694716, acc.: 50.00%] [G loss: 0.711121]\n",
      "522 [D loss: 0.693544, acc.: 53.12%] [G loss: 0.708864]\n",
      "523 [D loss: 0.694801, acc.: 53.12%] [G loss: 0.707515]\n",
      "524 [D loss: 0.697073, acc.: 46.88%] [G loss: 0.705320]\n",
      "525 [D loss: 0.694300, acc.: 40.62%] [G loss: 0.707111]\n",
      "526 [D loss: 0.694671, acc.: 43.75%] [G loss: 0.707098]\n",
      "527 [D loss: 0.693814, acc.: 43.75%] [G loss: 0.709373]\n",
      "528 [D loss: 0.692727, acc.: 50.00%] [G loss: 0.707066]\n",
      "529 [D loss: 0.696094, acc.: 46.88%] [G loss: 0.708165]\n",
      "530 [D loss: 0.692789, acc.: 54.69%] [G loss: 0.707794]\n",
      "531 [D loss: 0.693338, acc.: 53.12%] [G loss: 0.708202]\n",
      "532 [D loss: 0.697718, acc.: 37.50%] [G loss: 0.706411]\n",
      "533 [D loss: 0.695239, acc.: 48.44%] [G loss: 0.706766]\n",
      "534 [D loss: 0.695953, acc.: 43.75%] [G loss: 0.705667]\n",
      "535 [D loss: 0.696429, acc.: 48.44%] [G loss: 0.703415]\n",
      "536 [D loss: 0.695418, acc.: 45.31%] [G loss: 0.702693]\n",
      "537 [D loss: 0.695891, acc.: 40.62%] [G loss: 0.702925]\n",
      "538 [D loss: 0.695228, acc.: 45.31%] [G loss: 0.701681]\n",
      "539 [D loss: 0.693859, acc.: 43.75%] [G loss: 0.702632]\n",
      "540 [D loss: 0.693179, acc.: 53.12%] [G loss: 0.701271]\n",
      "541 [D loss: 0.694699, acc.: 46.88%] [G loss: 0.703358]\n",
      "542 [D loss: 0.692681, acc.: 54.69%] [G loss: 0.703221]\n",
      "543 [D loss: 0.693661, acc.: 45.31%] [G loss: 0.703462]\n",
      "544 [D loss: 0.693385, acc.: 53.12%] [G loss: 0.705237]\n",
      "545 [D loss: 0.692811, acc.: 51.56%] [G loss: 0.703266]\n",
      "546 [D loss: 0.691728, acc.: 46.88%] [G loss: 0.708041]\n",
      "547 [D loss: 0.692291, acc.: 46.88%] [G loss: 0.704481]\n",
      "548 [D loss: 0.695101, acc.: 45.31%] [G loss: 0.706100]\n",
      "549 [D loss: 0.693824, acc.: 46.88%] [G loss: 0.705746]\n",
      "550 [D loss: 0.690850, acc.: 56.25%] [G loss: 0.706134]\n",
      "551 [D loss: 0.694959, acc.: 50.00%] [G loss: 0.707452]\n",
      "552 [D loss: 0.692542, acc.: 51.56%] [G loss: 0.707033]\n",
      "553 [D loss: 0.693088, acc.: 50.00%] [G loss: 0.706552]\n",
      "554 [D loss: 0.693592, acc.: 43.75%] [G loss: 0.705818]\n",
      "555 [D loss: 0.693411, acc.: 51.56%] [G loss: 0.701061]\n",
      "556 [D loss: 0.694538, acc.: 48.44%] [G loss: 0.704154]\n",
      "557 [D loss: 0.693639, acc.: 48.44%] [G loss: 0.705261]\n",
      "558 [D loss: 0.694769, acc.: 39.06%] [G loss: 0.703971]\n",
      "559 [D loss: 0.696256, acc.: 46.88%] [G loss: 0.703667]\n",
      "560 [D loss: 0.694098, acc.: 45.31%] [G loss: 0.705965]\n",
      "561 [D loss: 0.692699, acc.: 51.56%] [G loss: 0.703538]\n",
      "562 [D loss: 0.694194, acc.: 51.56%] [G loss: 0.703332]\n",
      "563 [D loss: 0.692898, acc.: 37.50%] [G loss: 0.701360]\n",
      "564 [D loss: 0.693024, acc.: 51.56%] [G loss: 0.705891]\n",
      "565 [D loss: 0.694881, acc.: 53.12%] [G loss: 0.705864]\n",
      "566 [D loss: 0.694542, acc.: 51.56%] [G loss: 0.703123]\n",
      "567 [D loss: 0.698944, acc.: 42.19%] [G loss: 0.701522]\n",
      "568 [D loss: 0.695605, acc.: 42.19%] [G loss: 0.701290]\n",
      "569 [D loss: 0.691648, acc.: 48.44%] [G loss: 0.703059]\n",
      "570 [D loss: 0.695290, acc.: 56.25%] [G loss: 0.702843]\n",
      "571 [D loss: 0.697143, acc.: 42.19%] [G loss: 0.702961]\n",
      "572 [D loss: 0.697743, acc.: 42.19%] [G loss: 0.699931]\n",
      "573 [D loss: 0.694043, acc.: 53.12%] [G loss: 0.699879]\n",
      "574 [D loss: 0.694468, acc.: 42.19%] [G loss: 0.699767]\n",
      "575 [D loss: 0.696016, acc.: 42.19%] [G loss: 0.700875]\n",
      "576 [D loss: 0.693675, acc.: 50.00%] [G loss: 0.700484]\n",
      "577 [D loss: 0.693914, acc.: 43.75%] [G loss: 0.700850]\n",
      "578 [D loss: 0.693051, acc.: 50.00%] [G loss: 0.699142]\n",
      "579 [D loss: 0.693919, acc.: 42.19%] [G loss: 0.697789]\n",
      "580 [D loss: 0.693321, acc.: 51.56%] [G loss: 0.698872]\n",
      "581 [D loss: 0.691483, acc.: 59.38%] [G loss: 0.699159]\n",
      "582 [D loss: 0.694656, acc.: 48.44%] [G loss: 0.697782]\n",
      "583 [D loss: 0.692260, acc.: 48.44%] [G loss: 0.698585]\n",
      "584 [D loss: 0.691020, acc.: 54.69%] [G loss: 0.699999]\n",
      "585 [D loss: 0.694861, acc.: 43.75%] [G loss: 0.699619]\n",
      "586 [D loss: 0.694263, acc.: 42.19%] [G loss: 0.699686]\n",
      "587 [D loss: 0.694214, acc.: 42.19%] [G loss: 0.700599]\n",
      "588 [D loss: 0.695376, acc.: 40.62%] [G loss: 0.700498]\n",
      "589 [D loss: 0.693989, acc.: 43.75%] [G loss: 0.699185]\n",
      "590 [D loss: 0.692244, acc.: 57.81%] [G loss: 0.697934]\n",
      "591 [D loss: 0.695816, acc.: 37.50%] [G loss: 0.697167]\n",
      "592 [D loss: 0.691887, acc.: 54.69%] [G loss: 0.698907]\n",
      "593 [D loss: 0.693034, acc.: 45.31%] [G loss: 0.699465]\n",
      "594 [D loss: 0.693744, acc.: 48.44%] [G loss: 0.698878]\n",
      "595 [D loss: 0.694583, acc.: 46.88%] [G loss: 0.697841]\n",
      "596 [D loss: 0.692611, acc.: 50.00%] [G loss: 0.698181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 [D loss: 0.693890, acc.: 48.44%] [G loss: 0.699118]\n",
      "598 [D loss: 0.693841, acc.: 53.12%] [G loss: 0.700178]\n",
      "599 [D loss: 0.694766, acc.: 54.69%] [G loss: 0.698219]\n",
      "600 [D loss: 0.694260, acc.: 45.31%] [G loss: 0.697247]\n",
      "601 [D loss: 0.692740, acc.: 53.12%] [G loss: 0.697831]\n",
      "602 [D loss: 0.693183, acc.: 48.44%] [G loss: 0.698674]\n",
      "603 [D loss: 0.693568, acc.: 50.00%] [G loss: 0.696407]\n",
      "604 [D loss: 0.694341, acc.: 45.31%] [G loss: 0.696921]\n",
      "605 [D loss: 0.694589, acc.: 37.50%] [G loss: 0.697273]\n",
      "606 [D loss: 0.693855, acc.: 45.31%] [G loss: 0.698061]\n",
      "607 [D loss: 0.693404, acc.: 53.12%] [G loss: 0.697068]\n",
      "608 [D loss: 0.694996, acc.: 39.06%] [G loss: 0.696865]\n",
      "609 [D loss: 0.692595, acc.: 54.69%] [G loss: 0.696261]\n",
      "610 [D loss: 0.693153, acc.: 48.44%] [G loss: 0.695293]\n",
      "611 [D loss: 0.692366, acc.: 54.69%] [G loss: 0.695899]\n",
      "612 [D loss: 0.693592, acc.: 42.19%] [G loss: 0.694639]\n",
      "613 [D loss: 0.693544, acc.: 53.12%] [G loss: 0.697603]\n",
      "614 [D loss: 0.692099, acc.: 51.56%] [G loss: 0.695192]\n",
      "615 [D loss: 0.693055, acc.: 53.12%] [G loss: 0.696238]\n",
      "616 [D loss: 0.693642, acc.: 45.31%] [G loss: 0.695465]\n",
      "617 [D loss: 0.693522, acc.: 48.44%] [G loss: 0.694288]\n",
      "618 [D loss: 0.692721, acc.: 53.12%] [G loss: 0.695528]\n",
      "619 [D loss: 0.694812, acc.: 39.06%] [G loss: 0.695531]\n",
      "620 [D loss: 0.692554, acc.: 51.56%] [G loss: 0.695666]\n",
      "621 [D loss: 0.693387, acc.: 48.44%] [G loss: 0.695146]\n",
      "622 [D loss: 0.692354, acc.: 51.56%] [G loss: 0.695577]\n",
      "623 [D loss: 0.688836, acc.: 62.50%] [G loss: 0.694350]\n",
      "624 [D loss: 0.693385, acc.: 51.56%] [G loss: 0.695601]\n",
      "625 [D loss: 0.693837, acc.: 50.00%] [G loss: 0.693988]\n",
      "626 [D loss: 0.691934, acc.: 51.56%] [G loss: 0.693713]\n",
      "627 [D loss: 0.697123, acc.: 46.88%] [G loss: 0.692081]\n",
      "628 [D loss: 0.689479, acc.: 62.50%] [G loss: 0.690035]\n",
      "629 [D loss: 0.692840, acc.: 51.56%] [G loss: 0.688461]\n",
      "630 [D loss: 0.690110, acc.: 48.44%] [G loss: 0.690269]\n",
      "631 [D loss: 0.696136, acc.: 42.19%] [G loss: 0.692263]\n",
      "632 [D loss: 0.692070, acc.: 57.81%] [G loss: 0.691134]\n",
      "633 [D loss: 0.700119, acc.: 37.50%] [G loss: 0.689890]\n",
      "634 [D loss: 0.692567, acc.: 53.12%] [G loss: 0.690618]\n",
      "635 [D loss: 0.694995, acc.: 51.56%] [G loss: 0.690451]\n",
      "636 [D loss: 0.696342, acc.: 40.62%] [G loss: 0.689846]\n",
      "637 [D loss: 0.696506, acc.: 40.62%] [G loss: 0.692827]\n",
      "638 [D loss: 0.693661, acc.: 53.12%] [G loss: 0.690629]\n",
      "639 [D loss: 0.693536, acc.: 42.19%] [G loss: 0.691823]\n",
      "640 [D loss: 0.692128, acc.: 53.12%] [G loss: 0.690985]\n",
      "641 [D loss: 0.690257, acc.: 60.94%] [G loss: 0.693082]\n",
      "642 [D loss: 0.693623, acc.: 40.62%] [G loss: 0.691373]\n",
      "643 [D loss: 0.694297, acc.: 42.19%] [G loss: 0.691733]\n",
      "644 [D loss: 0.693788, acc.: 50.00%] [G loss: 0.692586]\n",
      "645 [D loss: 0.691286, acc.: 51.56%] [G loss: 0.692657]\n",
      "646 [D loss: 0.690246, acc.: 57.81%] [G loss: 0.690397]\n",
      "647 [D loss: 0.694749, acc.: 50.00%] [G loss: 0.692809]\n",
      "648 [D loss: 0.695081, acc.: 43.75%] [G loss: 0.689453]\n",
      "649 [D loss: 0.690997, acc.: 53.12%] [G loss: 0.691166]\n",
      "650 [D loss: 0.694274, acc.: 45.31%] [G loss: 0.691988]\n",
      "651 [D loss: 0.695152, acc.: 40.62%] [G loss: 0.691782]\n",
      "652 [D loss: 0.692258, acc.: 51.56%] [G loss: 0.691521]\n",
      "653 [D loss: 0.692834, acc.: 48.44%] [G loss: 0.690726]\n",
      "654 [D loss: 0.690701, acc.: 56.25%] [G loss: 0.692058]\n",
      "655 [D loss: 0.694423, acc.: 43.75%] [G loss: 0.690095]\n",
      "656 [D loss: 0.693719, acc.: 51.56%] [G loss: 0.690296]\n",
      "657 [D loss: 0.690574, acc.: 59.38%] [G loss: 0.689656]\n",
      "658 [D loss: 0.694612, acc.: 48.44%] [G loss: 0.691169]\n",
      "659 [D loss: 0.692059, acc.: 42.19%] [G loss: 0.687934]\n",
      "660 [D loss: 0.694327, acc.: 57.81%] [G loss: 0.688281]\n",
      "661 [D loss: 0.692401, acc.: 51.56%] [G loss: 0.689591]\n",
      "662 [D loss: 0.694762, acc.: 51.56%] [G loss: 0.691950]\n",
      "663 [D loss: 0.693289, acc.: 51.56%] [G loss: 0.692627]\n",
      "664 [D loss: 0.691289, acc.: 53.12%] [G loss: 0.692297]\n",
      "665 [D loss: 0.695009, acc.: 42.19%] [G loss: 0.691631]\n",
      "666 [D loss: 0.694990, acc.: 46.88%] [G loss: 0.692668]\n",
      "667 [D loss: 0.691133, acc.: 46.88%] [G loss: 0.690287]\n",
      "668 [D loss: 0.694995, acc.: 45.31%] [G loss: 0.692907]\n",
      "669 [D loss: 0.693484, acc.: 48.44%] [G loss: 0.690900]\n",
      "670 [D loss: 0.693597, acc.: 42.19%] [G loss: 0.693694]\n",
      "671 [D loss: 0.689826, acc.: 59.38%] [G loss: 0.691691]\n",
      "672 [D loss: 0.694587, acc.: 48.44%] [G loss: 0.692482]\n",
      "673 [D loss: 0.692050, acc.: 56.25%] [G loss: 0.690191]\n",
      "674 [D loss: 0.697187, acc.: 35.94%] [G loss: 0.691603]\n",
      "675 [D loss: 0.693838, acc.: 50.00%] [G loss: 0.692246]\n",
      "676 [D loss: 0.692600, acc.: 43.75%] [G loss: 0.689933]\n",
      "677 [D loss: 0.691712, acc.: 56.25%] [G loss: 0.692289]\n",
      "678 [D loss: 0.694719, acc.: 48.44%] [G loss: 0.691973]\n",
      "679 [D loss: 0.693627, acc.: 45.31%] [G loss: 0.690425]\n",
      "680 [D loss: 0.691369, acc.: 54.69%] [G loss: 0.689752]\n",
      "681 [D loss: 0.693969, acc.: 43.75%] [G loss: 0.692095]\n",
      "682 [D loss: 0.690094, acc.: 54.69%] [G loss: 0.692712]\n",
      "683 [D loss: 0.692011, acc.: 51.56%] [G loss: 0.690579]\n",
      "684 [D loss: 0.694254, acc.: 48.44%] [G loss: 0.689582]\n",
      "685 [D loss: 0.690363, acc.: 59.38%] [G loss: 0.686042]\n",
      "686 [D loss: 0.693233, acc.: 48.44%] [G loss: 0.689319]\n",
      "687 [D loss: 0.697079, acc.: 50.00%] [G loss: 0.689081]\n",
      "688 [D loss: 0.693040, acc.: 53.12%] [G loss: 0.689395]\n",
      "689 [D loss: 0.693021, acc.: 54.69%] [G loss: 0.689011]\n",
      "690 [D loss: 0.699019, acc.: 34.38%] [G loss: 0.690314]\n",
      "691 [D loss: 0.692005, acc.: 60.94%] [G loss: 0.688928]\n",
      "692 [D loss: 0.695593, acc.: 39.06%] [G loss: 0.688705]\n",
      "693 [D loss: 0.697156, acc.: 42.19%] [G loss: 0.690419]\n",
      "694 [D loss: 0.691667, acc.: 56.25%] [G loss: 0.690372]\n",
      "695 [D loss: 0.695427, acc.: 42.19%] [G loss: 0.690922]\n",
      "696 [D loss: 0.694644, acc.: 48.44%] [G loss: 0.689856]\n",
      "697 [D loss: 0.695363, acc.: 46.88%] [G loss: 0.691205]\n",
      "698 [D loss: 0.694452, acc.: 43.75%] [G loss: 0.692759]\n",
      "699 [D loss: 0.696095, acc.: 43.75%] [G loss: 0.691706]\n",
      "700 [D loss: 0.692868, acc.: 51.56%] [G loss: 0.690975]\n",
      "701 [D loss: 0.695477, acc.: 39.06%] [G loss: 0.692396]\n",
      "702 [D loss: 0.693003, acc.: 53.12%] [G loss: 0.695382]\n",
      "703 [D loss: 0.694233, acc.: 43.75%] [G loss: 0.694521]\n",
      "704 [D loss: 0.694267, acc.: 43.75%] [G loss: 0.693724]\n",
      "705 [D loss: 0.694314, acc.: 42.19%] [G loss: 0.695155]\n",
      "706 [D loss: 0.694016, acc.: 45.31%] [G loss: 0.696854]\n",
      "707 [D loss: 0.692947, acc.: 46.88%] [G loss: 0.693817]\n",
      "708 [D loss: 0.693368, acc.: 50.00%] [G loss: 0.694094]\n",
      "709 [D loss: 0.691865, acc.: 51.56%] [G loss: 0.695476]\n",
      "710 [D loss: 0.694389, acc.: 40.62%] [G loss: 0.695524]\n",
      "711 [D loss: 0.692815, acc.: 53.12%] [G loss: 0.693539]\n",
      "712 [D loss: 0.693068, acc.: 50.00%] [G loss: 0.695133]\n",
      "713 [D loss: 0.693871, acc.: 45.31%] [G loss: 0.698810]\n",
      "714 [D loss: 0.692493, acc.: 45.31%] [G loss: 0.696450]\n",
      "715 [D loss: 0.693855, acc.: 43.75%] [G loss: 0.698847]\n",
      "716 [D loss: 0.691433, acc.: 59.38%] [G loss: 0.699061]\n",
      "717 [D loss: 0.690594, acc.: 51.56%] [G loss: 0.702437]\n",
      "718 [D loss: 0.694436, acc.: 46.88%] [G loss: 0.700872]\n",
      "719 [D loss: 0.693652, acc.: 48.44%] [G loss: 0.702196]\n",
      "720 [D loss: 0.694295, acc.: 48.44%] [G loss: 0.700985]\n",
      "721 [D loss: 0.695046, acc.: 39.06%] [G loss: 0.697975]\n",
      "722 [D loss: 0.690167, acc.: 56.25%] [G loss: 0.703110]\n",
      "723 [D loss: 0.693156, acc.: 48.44%] [G loss: 0.698886]\n",
      "724 [D loss: 0.692754, acc.: 56.25%] [G loss: 0.698897]\n",
      "725 [D loss: 0.693953, acc.: 43.75%] [G loss: 0.698600]\n",
      "726 [D loss: 0.691828, acc.: 53.12%] [G loss: 0.701573]\n",
      "727 [D loss: 0.690564, acc.: 56.25%] [G loss: 0.699646]\n",
      "728 [D loss: 0.691856, acc.: 60.94%] [G loss: 0.702347]\n",
      "729 [D loss: 0.696027, acc.: 54.69%] [G loss: 0.697095]\n",
      "730 [D loss: 0.693228, acc.: 54.69%] [G loss: 0.701181]\n",
      "731 [D loss: 0.696869, acc.: 48.44%] [G loss: 0.700450]\n",
      "732 [D loss: 0.694817, acc.: 54.69%] [G loss: 0.698980]\n",
      "733 [D loss: 0.691187, acc.: 56.25%] [G loss: 0.698912]\n",
      "734 [D loss: 0.694832, acc.: 54.69%] [G loss: 0.700852]\n",
      "735 [D loss: 0.691864, acc.: 53.12%] [G loss: 0.699343]\n",
      "736 [D loss: 0.693435, acc.: 48.44%] [G loss: 0.698467]\n",
      "737 [D loss: 0.694303, acc.: 56.25%] [G loss: 0.698909]\n",
      "738 [D loss: 0.694135, acc.: 53.12%] [G loss: 0.701354]\n",
      "739 [D loss: 0.693157, acc.: 50.00%] [G loss: 0.700537]\n",
      "740 [D loss: 0.691584, acc.: 51.56%] [G loss: 0.699347]\n",
      "741 [D loss: 0.691521, acc.: 57.81%] [G loss: 0.699572]\n",
      "742 [D loss: 0.697410, acc.: 34.38%] [G loss: 0.698821]\n",
      "743 [D loss: 0.697441, acc.: 34.38%] [G loss: 0.698288]\n",
      "744 [D loss: 0.694333, acc.: 42.19%] [G loss: 0.698470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745 [D loss: 0.693102, acc.: 45.31%] [G loss: 0.695645]\n",
      "746 [D loss: 0.691685, acc.: 48.44%] [G loss: 0.696168]\n",
      "747 [D loss: 0.690920, acc.: 56.25%] [G loss: 0.695405]\n",
      "748 [D loss: 0.695504, acc.: 50.00%] [G loss: 0.694356]\n",
      "749 [D loss: 0.696446, acc.: 43.75%] [G loss: 0.694669]\n",
      "750 [D loss: 0.690715, acc.: 50.00%] [G loss: 0.693757]\n",
      "751 [D loss: 0.693760, acc.: 46.88%] [G loss: 0.695034]\n",
      "752 [D loss: 0.693100, acc.: 56.25%] [G loss: 0.695288]\n",
      "753 [D loss: 0.692969, acc.: 50.00%] [G loss: 0.694506]\n",
      "754 [D loss: 0.693032, acc.: 56.25%] [G loss: 0.694663]\n",
      "755 [D loss: 0.692319, acc.: 53.12%] [G loss: 0.695091]\n",
      "756 [D loss: 0.693936, acc.: 50.00%] [G loss: 0.694843]\n",
      "757 [D loss: 0.693869, acc.: 50.00%] [G loss: 0.694732]\n",
      "758 [D loss: 0.692513, acc.: 53.12%] [G loss: 0.693874]\n",
      "759 [D loss: 0.694362, acc.: 46.88%] [G loss: 0.696176]\n",
      "760 [D loss: 0.693945, acc.: 46.88%] [G loss: 0.695969]\n",
      "761 [D loss: 0.692860, acc.: 50.00%] [G loss: 0.697619]\n",
      "762 [D loss: 0.692528, acc.: 53.12%] [G loss: 0.698240]\n",
      "763 [D loss: 0.695864, acc.: 48.44%] [G loss: 0.698202]\n",
      "764 [D loss: 0.694737, acc.: 42.19%] [G loss: 0.698267]\n",
      "765 [D loss: 0.696169, acc.: 35.94%] [G loss: 0.695258]\n",
      "766 [D loss: 0.693888, acc.: 43.75%] [G loss: 0.696930]\n",
      "767 [D loss: 0.693837, acc.: 43.75%] [G loss: 0.697762]\n",
      "768 [D loss: 0.692182, acc.: 51.56%] [G loss: 0.698995]\n",
      "769 [D loss: 0.693199, acc.: 53.12%] [G loss: 0.696474]\n",
      "770 [D loss: 0.693173, acc.: 51.56%] [G loss: 0.698479]\n",
      "771 [D loss: 0.693848, acc.: 50.00%] [G loss: 0.698600]\n",
      "772 [D loss: 0.693046, acc.: 54.69%] [G loss: 0.697766]\n",
      "773 [D loss: 0.691570, acc.: 54.69%] [G loss: 0.696783]\n",
      "774 [D loss: 0.694018, acc.: 42.19%] [G loss: 0.697616]\n",
      "775 [D loss: 0.691879, acc.: 50.00%] [G loss: 0.699409]\n",
      "776 [D loss: 0.690996, acc.: 57.81%] [G loss: 0.698944]\n",
      "777 [D loss: 0.690884, acc.: 59.38%] [G loss: 0.699750]\n",
      "778 [D loss: 0.693250, acc.: 46.88%] [G loss: 0.696896]\n",
      "779 [D loss: 0.694277, acc.: 45.31%] [G loss: 0.701602]\n",
      "780 [D loss: 0.691716, acc.: 48.44%] [G loss: 0.700773]\n",
      "781 [D loss: 0.695240, acc.: 37.50%] [G loss: 0.698118]\n",
      "782 [D loss: 0.693781, acc.: 51.56%] [G loss: 0.697661]\n",
      "783 [D loss: 0.693322, acc.: 51.56%] [G loss: 0.694085]\n",
      "784 [D loss: 0.692403, acc.: 53.12%] [G loss: 0.695817]\n",
      "785 [D loss: 0.692701, acc.: 53.12%] [G loss: 0.696275]\n",
      "786 [D loss: 0.694319, acc.: 50.00%] [G loss: 0.698446]\n",
      "787 [D loss: 0.693775, acc.: 53.12%] [G loss: 0.696668]\n",
      "788 [D loss: 0.693344, acc.: 53.12%] [G loss: 0.696591]\n",
      "789 [D loss: 0.690540, acc.: 64.06%] [G loss: 0.696612]\n",
      "790 [D loss: 0.694425, acc.: 43.75%] [G loss: 0.698406]\n",
      "791 [D loss: 0.693048, acc.: 56.25%] [G loss: 0.699765]\n",
      "792 [D loss: 0.692508, acc.: 57.81%] [G loss: 0.698021]\n",
      "793 [D loss: 0.690089, acc.: 59.38%] [G loss: 0.697754]\n",
      "794 [D loss: 0.690203, acc.: 60.94%] [G loss: 0.697582]\n",
      "795 [D loss: 0.693825, acc.: 50.00%] [G loss: 0.697181]\n",
      "796 [D loss: 0.693122, acc.: 59.38%] [G loss: 0.693374]\n",
      "797 [D loss: 0.694492, acc.: 48.44%] [G loss: 0.697758]\n",
      "798 [D loss: 0.694173, acc.: 45.31%] [G loss: 0.697076]\n",
      "799 [D loss: 0.694757, acc.: 43.75%] [G loss: 0.696087]\n",
      "800 [D loss: 0.694473, acc.: 46.88%] [G loss: 0.698389]\n",
      "801 [D loss: 0.691658, acc.: 46.88%] [G loss: 0.695804]\n",
      "802 [D loss: 0.695690, acc.: 40.62%] [G loss: 0.698368]\n",
      "803 [D loss: 0.689775, acc.: 59.38%] [G loss: 0.695042]\n",
      "804 [D loss: 0.694009, acc.: 53.12%] [G loss: 0.696800]\n",
      "805 [D loss: 0.692078, acc.: 51.56%] [G loss: 0.696582]\n",
      "806 [D loss: 0.692326, acc.: 53.12%] [G loss: 0.697609]\n",
      "807 [D loss: 0.694428, acc.: 53.12%] [G loss: 0.697754]\n",
      "808 [D loss: 0.693288, acc.: 45.31%] [G loss: 0.697394]\n",
      "809 [D loss: 0.692781, acc.: 46.88%] [G loss: 0.701431]\n",
      "810 [D loss: 0.695595, acc.: 43.75%] [G loss: 0.701429]\n",
      "811 [D loss: 0.692657, acc.: 56.25%] [G loss: 0.700271]\n",
      "812 [D loss: 0.696526, acc.: 40.62%] [G loss: 0.699686]\n",
      "813 [D loss: 0.691881, acc.: 50.00%] [G loss: 0.699520]\n",
      "814 [D loss: 0.693282, acc.: 50.00%] [G loss: 0.700822]\n",
      "815 [D loss: 0.691687, acc.: 57.81%] [G loss: 0.700061]\n",
      "816 [D loss: 0.692983, acc.: 45.31%] [G loss: 0.702385]\n",
      "817 [D loss: 0.693292, acc.: 54.69%] [G loss: 0.701777]\n",
      "818 [D loss: 0.694871, acc.: 45.31%] [G loss: 0.699913]\n",
      "819 [D loss: 0.693671, acc.: 43.75%] [G loss: 0.698506]\n",
      "820 [D loss: 0.693432, acc.: 50.00%] [G loss: 0.699533]\n",
      "821 [D loss: 0.693810, acc.: 45.31%] [G loss: 0.699661]\n",
      "822 [D loss: 0.694524, acc.: 42.19%] [G loss: 0.700604]\n",
      "823 [D loss: 0.693122, acc.: 57.81%] [G loss: 0.697218]\n",
      "824 [D loss: 0.691152, acc.: 57.81%] [G loss: 0.700161]\n",
      "825 [D loss: 0.695883, acc.: 35.94%] [G loss: 0.702354]\n",
      "826 [D loss: 0.693042, acc.: 45.31%] [G loss: 0.700520]\n",
      "827 [D loss: 0.694023, acc.: 48.44%] [G loss: 0.702433]\n",
      "828 [D loss: 0.693455, acc.: 43.75%] [G loss: 0.700771]\n",
      "829 [D loss: 0.692315, acc.: 53.12%] [G loss: 0.703730]\n",
      "830 [D loss: 0.694468, acc.: 48.44%] [G loss: 0.701315]\n",
      "831 [D loss: 0.692107, acc.: 51.56%] [G loss: 0.701899]\n",
      "832 [D loss: 0.694942, acc.: 43.75%] [G loss: 0.700684]\n",
      "833 [D loss: 0.693432, acc.: 50.00%] [G loss: 0.701564]\n",
      "834 [D loss: 0.693236, acc.: 43.75%] [G loss: 0.700312]\n",
      "835 [D loss: 0.695152, acc.: 45.31%] [G loss: 0.701243]\n",
      "836 [D loss: 0.694089, acc.: 43.75%] [G loss: 0.700784]\n",
      "837 [D loss: 0.693227, acc.: 51.56%] [G loss: 0.701497]\n",
      "838 [D loss: 0.694098, acc.: 40.62%] [G loss: 0.701542]\n",
      "839 [D loss: 0.693010, acc.: 54.69%] [G loss: 0.702692]\n",
      "840 [D loss: 0.691642, acc.: 56.25%] [G loss: 0.702328]\n",
      "841 [D loss: 0.694639, acc.: 42.19%] [G loss: 0.701373]\n",
      "842 [D loss: 0.692586, acc.: 51.56%] [G loss: 0.700529]\n",
      "843 [D loss: 0.694249, acc.: 48.44%] [G loss: 0.702807]\n",
      "844 [D loss: 0.694002, acc.: 43.75%] [G loss: 0.702017]\n",
      "845 [D loss: 0.691728, acc.: 54.69%] [G loss: 0.701354]\n",
      "846 [D loss: 0.693827, acc.: 50.00%] [G loss: 0.703469]\n",
      "847 [D loss: 0.693575, acc.: 46.88%] [G loss: 0.701235]\n",
      "848 [D loss: 0.696483, acc.: 50.00%] [G loss: 0.700183]\n",
      "849 [D loss: 0.692111, acc.: 59.38%] [G loss: 0.703272]\n",
      "850 [D loss: 0.694860, acc.: 54.69%] [G loss: 0.703033]\n",
      "851 [D loss: 0.693422, acc.: 42.19%] [G loss: 0.699936]\n",
      "852 [D loss: 0.693711, acc.: 53.12%] [G loss: 0.701449]\n",
      "853 [D loss: 0.693763, acc.: 46.88%] [G loss: 0.698507]\n",
      "854 [D loss: 0.692465, acc.: 62.50%] [G loss: 0.700455]\n",
      "855 [D loss: 0.695208, acc.: 48.44%] [G loss: 0.700335]\n",
      "856 [D loss: 0.694096, acc.: 48.44%] [G loss: 0.700610]\n",
      "857 [D loss: 0.692911, acc.: 54.69%] [G loss: 0.698253]\n",
      "858 [D loss: 0.692428, acc.: 53.12%] [G loss: 0.699242]\n",
      "859 [D loss: 0.690252, acc.: 60.94%] [G loss: 0.700591]\n",
      "860 [D loss: 0.693584, acc.: 45.31%] [G loss: 0.700292]\n",
      "861 [D loss: 0.694688, acc.: 46.88%] [G loss: 0.700763]\n",
      "862 [D loss: 0.694444, acc.: 46.88%] [G loss: 0.701963]\n",
      "863 [D loss: 0.693324, acc.: 50.00%] [G loss: 0.699175]\n",
      "864 [D loss: 0.691944, acc.: 59.38%] [G loss: 0.700720]\n",
      "865 [D loss: 0.692105, acc.: 64.06%] [G loss: 0.701298]\n",
      "866 [D loss: 0.693513, acc.: 46.88%] [G loss: 0.699161]\n",
      "867 [D loss: 0.693139, acc.: 53.12%] [G loss: 0.699698]\n",
      "868 [D loss: 0.694253, acc.: 37.50%] [G loss: 0.698002]\n",
      "869 [D loss: 0.693959, acc.: 46.88%] [G loss: 0.698189]\n",
      "870 [D loss: 0.692529, acc.: 56.25%] [G loss: 0.698300]\n",
      "871 [D loss: 0.693522, acc.: 53.12%] [G loss: 0.699198]\n",
      "872 [D loss: 0.692698, acc.: 46.88%] [G loss: 0.698380]\n",
      "873 [D loss: 0.694697, acc.: 46.88%] [G loss: 0.697599]\n",
      "874 [D loss: 0.694017, acc.: 51.56%] [G loss: 0.696947]\n",
      "875 [D loss: 0.691597, acc.: 50.00%] [G loss: 0.698887]\n",
      "876 [D loss: 0.693264, acc.: 46.88%] [G loss: 0.698292]\n",
      "877 [D loss: 0.693134, acc.: 57.81%] [G loss: 0.699181]\n",
      "878 [D loss: 0.693036, acc.: 56.25%] [G loss: 0.698388]\n",
      "879 [D loss: 0.692409, acc.: 51.56%] [G loss: 0.698136]\n",
      "880 [D loss: 0.694476, acc.: 43.75%] [G loss: 0.697893]\n",
      "881 [D loss: 0.694194, acc.: 51.56%] [G loss: 0.696287]\n",
      "882 [D loss: 0.693305, acc.: 54.69%] [G loss: 0.698683]\n",
      "883 [D loss: 0.693284, acc.: 43.75%] [G loss: 0.696538]\n",
      "884 [D loss: 0.693948, acc.: 53.12%] [G loss: 0.699183]\n",
      "885 [D loss: 0.692613, acc.: 53.12%] [G loss: 0.696318]\n",
      "886 [D loss: 0.694003, acc.: 48.44%] [G loss: 0.696878]\n",
      "887 [D loss: 0.691577, acc.: 60.94%] [G loss: 0.696985]\n",
      "888 [D loss: 0.694098, acc.: 46.88%] [G loss: 0.697656]\n",
      "889 [D loss: 0.693261, acc.: 53.12%] [G loss: 0.698049]\n",
      "890 [D loss: 0.692893, acc.: 54.69%] [G loss: 0.697590]\n",
      "891 [D loss: 0.693413, acc.: 54.69%] [G loss: 0.696623]\n",
      "892 [D loss: 0.694879, acc.: 46.88%] [G loss: 0.697677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893 [D loss: 0.691224, acc.: 59.38%] [G loss: 0.698221]\n",
      "894 [D loss: 0.692094, acc.: 57.81%] [G loss: 0.698463]\n",
      "895 [D loss: 0.692227, acc.: 56.25%] [G loss: 0.698214]\n",
      "896 [D loss: 0.694839, acc.: 54.69%] [G loss: 0.697037]\n",
      "897 [D loss: 0.692653, acc.: 50.00%] [G loss: 0.696693]\n",
      "898 [D loss: 0.693347, acc.: 51.56%] [G loss: 0.699200]\n",
      "899 [D loss: 0.693864, acc.: 37.50%] [G loss: 0.697201]\n",
      "900 [D loss: 0.692615, acc.: 51.56%] [G loss: 0.697744]\n",
      "901 [D loss: 0.693577, acc.: 53.12%] [G loss: 0.696811]\n",
      "902 [D loss: 0.691587, acc.: 51.56%] [G loss: 0.696663]\n",
      "903 [D loss: 0.692979, acc.: 45.31%] [G loss: 0.698012]\n",
      "904 [D loss: 0.693904, acc.: 40.62%] [G loss: 0.697274]\n",
      "905 [D loss: 0.693742, acc.: 43.75%] [G loss: 0.697378]\n",
      "906 [D loss: 0.693260, acc.: 48.44%] [G loss: 0.697222]\n",
      "907 [D loss: 0.691517, acc.: 56.25%] [G loss: 0.696316]\n",
      "908 [D loss: 0.693282, acc.: 50.00%] [G loss: 0.696676]\n",
      "909 [D loss: 0.695362, acc.: 50.00%] [G loss: 0.694923]\n",
      "910 [D loss: 0.693833, acc.: 48.44%] [G loss: 0.697650]\n",
      "911 [D loss: 0.696667, acc.: 31.25%] [G loss: 0.696634]\n",
      "912 [D loss: 0.694841, acc.: 39.06%] [G loss: 0.697862]\n",
      "913 [D loss: 0.694496, acc.: 48.44%] [G loss: 0.698287]\n",
      "914 [D loss: 0.693951, acc.: 42.19%] [G loss: 0.698545]\n",
      "915 [D loss: 0.693465, acc.: 48.44%] [G loss: 0.696652]\n",
      "916 [D loss: 0.693560, acc.: 48.44%] [G loss: 0.697076]\n",
      "917 [D loss: 0.693528, acc.: 50.00%] [G loss: 0.697640]\n",
      "918 [D loss: 0.694104, acc.: 43.75%] [G loss: 0.695924]\n",
      "919 [D loss: 0.693205, acc.: 51.56%] [G loss: 0.697453]\n",
      "920 [D loss: 0.694379, acc.: 43.75%] [G loss: 0.696761]\n",
      "921 [D loss: 0.692423, acc.: 53.12%] [G loss: 0.696536]\n",
      "922 [D loss: 0.694266, acc.: 45.31%] [G loss: 0.698465]\n",
      "923 [D loss: 0.692978, acc.: 56.25%] [G loss: 0.697091]\n",
      "924 [D loss: 0.693646, acc.: 46.88%] [G loss: 0.697184]\n",
      "925 [D loss: 0.692417, acc.: 51.56%] [G loss: 0.697338]\n",
      "926 [D loss: 0.693889, acc.: 46.88%] [G loss: 0.697063]\n",
      "927 [D loss: 0.693500, acc.: 56.25%] [G loss: 0.696907]\n",
      "928 [D loss: 0.692779, acc.: 51.56%] [G loss: 0.696308]\n",
      "929 [D loss: 0.692850, acc.: 45.31%] [G loss: 0.696798]\n",
      "930 [D loss: 0.692981, acc.: 56.25%] [G loss: 0.695789]\n",
      "931 [D loss: 0.691404, acc.: 67.19%] [G loss: 0.697508]\n",
      "932 [D loss: 0.695293, acc.: 46.88%] [G loss: 0.698142]\n",
      "933 [D loss: 0.693835, acc.: 46.88%] [G loss: 0.697022]\n",
      "934 [D loss: 0.693596, acc.: 51.56%] [G loss: 0.697385]\n",
      "935 [D loss: 0.692321, acc.: 57.81%] [G loss: 0.696499]\n",
      "936 [D loss: 0.693685, acc.: 45.31%] [G loss: 0.696475]\n",
      "937 [D loss: 0.691838, acc.: 54.69%] [G loss: 0.694807]\n",
      "938 [D loss: 0.692223, acc.: 43.75%] [G loss: 0.695948]\n",
      "939 [D loss: 0.691703, acc.: 59.38%] [G loss: 0.696446]\n",
      "940 [D loss: 0.693026, acc.: 51.56%] [G loss: 0.693774]\n",
      "941 [D loss: 0.691826, acc.: 48.44%] [G loss: 0.694174]\n",
      "942 [D loss: 0.693800, acc.: 45.31%] [G loss: 0.693449]\n",
      "943 [D loss: 0.694558, acc.: 37.50%] [G loss: 0.696223]\n",
      "944 [D loss: 0.692534, acc.: 57.81%] [G loss: 0.695133]\n",
      "945 [D loss: 0.694690, acc.: 53.12%] [G loss: 0.694339]\n",
      "946 [D loss: 0.692814, acc.: 51.56%] [G loss: 0.696807]\n",
      "947 [D loss: 0.693620, acc.: 45.31%] [G loss: 0.695803]\n",
      "948 [D loss: 0.692685, acc.: 50.00%] [G loss: 0.696632]\n",
      "949 [D loss: 0.693211, acc.: 50.00%] [G loss: 0.698307]\n",
      "950 [D loss: 0.693412, acc.: 51.56%] [G loss: 0.695963]\n",
      "951 [D loss: 0.693378, acc.: 53.12%] [G loss: 0.699086]\n",
      "952 [D loss: 0.693159, acc.: 45.31%] [G loss: 0.698074]\n",
      "953 [D loss: 0.690958, acc.: 64.06%] [G loss: 0.697494]\n",
      "954 [D loss: 0.691437, acc.: 54.69%] [G loss: 0.694063]\n",
      "955 [D loss: 0.694778, acc.: 43.75%] [G loss: 0.697609]\n",
      "956 [D loss: 0.692333, acc.: 54.69%] [G loss: 0.697268]\n",
      "957 [D loss: 0.692678, acc.: 53.12%] [G loss: 0.697493]\n",
      "958 [D loss: 0.692153, acc.: 50.00%] [G loss: 0.698073]\n",
      "959 [D loss: 0.695641, acc.: 39.06%] [G loss: 0.698948]\n",
      "960 [D loss: 0.693900, acc.: 45.31%] [G loss: 0.697163]\n",
      "961 [D loss: 0.695211, acc.: 46.88%] [G loss: 0.697678]\n",
      "962 [D loss: 0.693171, acc.: 56.25%] [G loss: 0.698746]\n",
      "963 [D loss: 0.693021, acc.: 53.12%] [G loss: 0.698205]\n",
      "964 [D loss: 0.692871, acc.: 45.31%] [G loss: 0.699840]\n",
      "965 [D loss: 0.693783, acc.: 43.75%] [G loss: 0.697668]\n",
      "966 [D loss: 0.692620, acc.: 59.38%] [G loss: 0.698184]\n",
      "967 [D loss: 0.691533, acc.: 53.12%] [G loss: 0.698605]\n",
      "968 [D loss: 0.691849, acc.: 56.25%] [G loss: 0.697091]\n",
      "969 [D loss: 0.693241, acc.: 48.44%] [G loss: 0.697715]\n",
      "970 [D loss: 0.692240, acc.: 57.81%] [G loss: 0.697997]\n",
      "971 [D loss: 0.694934, acc.: 45.31%] [G loss: 0.696277]\n",
      "972 [D loss: 0.690755, acc.: 54.69%] [G loss: 0.698763]\n",
      "973 [D loss: 0.697430, acc.: 28.12%] [G loss: 0.698341]\n",
      "974 [D loss: 0.694009, acc.: 43.75%] [G loss: 0.697835]\n",
      "975 [D loss: 0.694219, acc.: 48.44%] [G loss: 0.697428]\n",
      "976 [D loss: 0.690909, acc.: 54.69%] [G loss: 0.697615]\n",
      "977 [D loss: 0.694858, acc.: 42.19%] [G loss: 0.696705]\n",
      "978 [D loss: 0.694437, acc.: 46.88%] [G loss: 0.697191]\n",
      "979 [D loss: 0.694324, acc.: 45.31%] [G loss: 0.697087]\n",
      "980 [D loss: 0.692704, acc.: 53.12%] [G loss: 0.697584]\n",
      "981 [D loss: 0.692941, acc.: 50.00%] [G loss: 0.695641]\n",
      "982 [D loss: 0.693730, acc.: 46.88%] [G loss: 0.695198]\n",
      "983 [D loss: 0.693720, acc.: 35.94%] [G loss: 0.694055]\n",
      "984 [D loss: 0.692370, acc.: 54.69%] [G loss: 0.695526]\n",
      "985 [D loss: 0.691411, acc.: 64.06%] [G loss: 0.697288]\n",
      "986 [D loss: 0.694629, acc.: 45.31%] [G loss: 0.697790]\n",
      "987 [D loss: 0.694579, acc.: 40.62%] [G loss: 0.694802]\n",
      "988 [D loss: 0.693072, acc.: 43.75%] [G loss: 0.693947]\n",
      "989 [D loss: 0.693292, acc.: 54.69%] [G loss: 0.697680]\n",
      "990 [D loss: 0.693974, acc.: 42.19%] [G loss: 0.694714]\n",
      "991 [D loss: 0.692256, acc.: 57.81%] [G loss: 0.694490]\n",
      "992 [D loss: 0.692751, acc.: 53.12%] [G loss: 0.697732]\n",
      "993 [D loss: 0.696227, acc.: 35.94%] [G loss: 0.698765]\n",
      "994 [D loss: 0.692705, acc.: 48.44%] [G loss: 0.695793]\n",
      "995 [D loss: 0.691271, acc.: 54.69%] [G loss: 0.697093]\n",
      "996 [D loss: 0.692482, acc.: 50.00%] [G loss: 0.697684]\n",
      "997 [D loss: 0.693445, acc.: 43.75%] [G loss: 0.696028]\n",
      "998 [D loss: 0.693849, acc.: 45.31%] [G loss: 0.701191]\n",
      "999 [D loss: 0.692538, acc.: 48.44%] [G loss: 0.694283]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(rows=50)    \n",
    "gan.train(epochs=1000, batch_size=32, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
